# docker autoML
<b>inspired by Sberbank Data Science Journey 2018</b>

Система AutoML (автоматическое машинное обучение) на табличных данных. Для своей работы требует лишь предварительный анализ имеющихся данных - разметка признаков по типам. Позволяет устанавливать ограничения по памяти и времени обучения.
Решение создано по мотивам соревнований Sberbank Data Science Journey 2018.

Для работы используется библиотека [auto-sklearn](https://github.com/automl/auto-sklearn).
Ведется разработка альтернативных режимов работы.

Весь процесс построения и использования модели происходит в docker-контейнере, т.е. установка специфичных и платформо-зависимых библиотек не требуется.
Для построения и работы модели необходимы `python`, `make` и `docker`.
Для запуска скрипта разметки данных необходимы `python` и `pandas`.

## Make-команды для работы

`make model-train-regression` - обучение модели на датасете, задача регрессии.<br>
`make model-train-classification` - обучение модели на датасете, задача классификации.<br>
`make model-predict` - валидация модели на тестовых данных и запись файла с предсказаниями.<br>
`make data-prepare` - запуск скрипта автоматической разметки данных<br>
`make docker-build` - сборка Docker-образа, используя конфигурационный файл [Dockerfile](./Dockerfile).<br>
`make docker-push` - отправка Docker-образа на DockerHub.<br>
`make run-bash` - запустить терминал в Docker-контейнере.<br>
`make run-jupyter` - запустить Jupyter в Docker-контейнере по адресу http://localhost:8888.<br>

## Порядок работы

* При необходимости (кастомный docker-образ):
	* Запустить сборку образа `make docker-build`.
	* Залить Docker-образ на Docker Hub `make docker-push`.
* Для разметки признаков - отредактировать настройки [data_prepare.json](./settings/data_prepare.json) и запустить срипт автоматической разметки.
* Отредактировать настройки [automl.json](./settings/automl.json).
* Пользуемся!

## Описание настроек

* [automl.json](./settings/automl.json)<br>
`model/name` - имя модели, будет использовано для хранения в папке `models`<br>
`docker/image` - имя docker контейнера на DockerHub<br>
`solver/type` - используемый метод; допустимые значения - `'auto-sklearn'`<br>
`solver/time_limit_sec` - временной лимит на обучение (сек)<br>
`solver/memory_limit_mb` - лимит на объем используемой памяти (Мб)<br>
`solver/metrics/classification` - метрика качества в задаче классификации<br>
`solver/metrics/classification_need_proba` - флаг вероятностных предсказаний в задаче классификации<br>
`solver/metrics/regression` - метрика качества в задаче регрессии<br>
`data/path` - путь к папке с данными<br>
`data/train_file` - имя файла с тренировочным датасетом<br>
`data/test_file` - имя файла с валидационным датасетом<br>
`data/validation_file` - имя файла с истинными значениями целевой переменной, используется для подсчета метрики качества на валидационном датасете (`'null'` в случае, когда подсчет не требуется)<br>
`data/save_processed_data` - флаг сохранения датасета с обработанными признаками (после feature engineering этапа)<br>

* [data_prepare.json](./settings/data_prepare.json)<br>
`input_path` - путь к исходному датасету<br>
`output_path` - путь сохранения размеченного датасета<br>
`reader_params` - параметры метода `pd.read_csv`, которым читаем исходный датасет<br>
`writer_params` - параметры метода `df.to_csv`, которым пишем размеченный датасет (изменять не надо)<br>
`data_markup_params/target_column` - имя целевого признака в исходном датасете<br>
`data_markup_params/feature_names_mapping` - списки имен признаков в исходном датасете по их типам; неизвестные признаки перед обучением удаляются<br>
`data_markup_params/feature_values_transformers` - необходимые переименования значений признаков в исходном датасете<br>

## Требования к разметке признаков

* Индекс-столбец - `line_id`
* Целевая переменная - `target`
* Категориальный признак - префикс `cat_`
* Числовой признак (количественный или бинарный) - префикс `number_`
* Строковый признак - префикс `string_`
* Дата или дата/время в строковом формате - префикс `datetime_`

## Требования к датасетам (тренировочный и валидационный)

* Формат файла с данными - csv
* Кодировка текста - UTF-8
* Разделитель - `','`
* Чтение происходит методом `read_csv` библиотеки pandas
