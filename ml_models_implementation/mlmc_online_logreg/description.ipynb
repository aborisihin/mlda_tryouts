{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Логистическая регрессия в задаче классификации текстов</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Автор материала: Алексей Борисихин"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализована модель multiclass/multilabel классификации текстов, использующая онлайн обучение (стохастический градиентный спуск с логистической функцией потерь). В качестве примера взят датасет вопросов на stackoverflow.com, задачей стоит предсказание тегов вопросов.\n",
    "\n",
    "Основным преимуществом онлайн обучения является возможность не загружать весь обучающий датасет в память, что позволяет использовать большие датасеты для обучения. Модель обучатеся итеративно по каждому объекту выборки. Соответственно, не составляет труда дообучить модель на новых данных.\n",
    "\n",
    "Данные доступны для скачивания по ссылке: https://yadi.sk/d/Vvc4JmX13ZdWvk (готовая выборка из 125k объектов).<br>\n",
    "Используя парсинг страниц сайта stackoverflow.com, можно было бы увеличить размер имеющихся данных. Но, ввиду ограничений сайта на количество запросов, представляется фомировать только небольшие датасеты за раз. Их можно использовать для дообучения модели.\n",
    "\n",
    "За основу было взято задание курса [OpenDataScience](https://github.com/Yorko/mlcourse_open) (весна 2018). Автор задания - Павел Нестеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формат входных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входные данные представляют собой файл с вопросами пользователей stackoverflow.com. В каждой строке файла содержится текст вопроса и список тегов, разделенные символом табуляции. Теги в списке разделены пробелом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание математической основы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Логистическая регрессия для двух классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим логистическую регрессию для двух классов $\\{0, 1\\}$. Обозначим вектор признаков объекта как $\\textbf{x}$. Вероятность принадлежности объекта классу $1$, вспомнив теорему Байеса, можно записать как:\n",
    "\n",
    "$$\n",
    "p\\left(c = 1 \\mid \\textbf{x}\\right) = \n",
    "\\dfrac\n",
    "{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right)}\n",
    "{p\\left(\\textbf{x}\\right)}\n",
    "$$\n",
    "\n",
    "Воспользуясь формулой полной вероятности, получаем:\n",
    "\n",
    "$$\n",
    "p\\left(c = 1 \\mid \\textbf{x}\\right) = \n",
    "\\dfrac\n",
    "{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right)}\n",
    "{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\textbf{x} \\mid c = 0\\right)p\\left(c = 0\\right)}\n",
    "$$\n",
    "\n",
    "Введя параметр:\n",
    "\n",
    "$$\n",
    "a = \\log \\dfrac\n",
    "{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right)}\n",
    "{p\\left(\\textbf{x} \\mid c = 0\\right)p\\left(c = 0\\right)}\n",
    "$$\n",
    "\n",
    "Мы можем наше выражение переписать в виде:\n",
    "\n",
    "$$\n",
    "p\\left(c = 1 \\mid \\textbf{x}\\right) = \\dfrac{1}{1 + e^{-a}} = \\sigma\\left(a\\right)\n",
    "$$\n",
    "\n",
    "где $\\sigma\\left(a\\right)$ - обозначение функции логистического сигмоида для скалярного аргумента.\n",
    "\n",
    "Значение же параметра $a$ мы моделируем линейной функцией от признаков объекта и параметров модели:\n",
    "\n",
    "$$\n",
    "a = \\sum_{i=0}^M w_i x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача многоклассовой классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обобщим подход до задачи многоклассовой классификации. У нас есть $K$ классов, к которым может принадлежать объект: $\\{1, 2, ..., K\\}$. Запишем вероятность принадлежности объекта классу $k$:\n",
    "\n",
    "$$\n",
    "p\\left(c = k \\mid \\textbf{x}\\right) = \n",
    "\\dfrac\n",
    "{p\\left(\\textbf{x} \\mid c = k\\right)p\\left(c = k\\right)}\n",
    "{p\\left(\\textbf{x}\\right)} =\n",
    "\\dfrac\n",
    "{p\\left(\\textbf{x} \\mid c = k\\right)p\\left(c = k\\right)}\n",
    "{\\sum_{i=1}^Kp\\left(\\textbf{x} \\mid c = i\\right)p\\left(c = i\\right)}\n",
    "$$\n",
    "\n",
    "Введем параметр:\n",
    "\n",
    "$$\n",
    "z_k = \\log p\\left(\\textbf{x} \\mid c=k \\right) p\\left(c = k\\right)\n",
    "$$\n",
    "\n",
    "И перепишем наше выражение в виде:\n",
    "\n",
    "$$\n",
    "p\\left(c = k \\mid \\textbf{x}\\right) =\n",
    "\\dfrac\n",
    "{e^{z_k}}\n",
    "{\\sum_{i=1}^K e^{z_i}}\n",
    "= \\sigma_k(\\textbf{z})\n",
    "$$\n",
    "\n",
    "где $\\sigma_k$ — $k$-ый компонент функции softmax (обобщение логистической регрессии для многомерного случая) при векторном аргументе. \n",
    "\n",
    "Вектор $\\sigma$ образует дискретное вероятностное распределение, т.е. $\\sum_{i=1}^{K}\\sigma_i = 1$.\n",
    "\n",
    "Значение параметра $z_k$ мы моделируем линейной функцией от признаков объекта (размерности M) и параметров модели для класса $k$:\n",
    "\n",
    "$$\n",
    "z_k = \\sum_{i=1}^Mw_{ki}x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для моделирования искомого распределения будем использовать [категориальное распределение](https://en.wikipedia.org/wiki/Categorical_distribution). Запишем функцию правдоподобия:\n",
    "\n",
    "$$\n",
    "L\\left(\\theta \\mid \\textbf{x}, \\textbf{y}\\right) = \n",
    "\\prod_{i=1}^{K}p_{i}^{y_{i}} = \n",
    "\\prod_{i=1}^{K}\\sigma_{i}(\\textbf{z})^{y_{i}}\n",
    "$$\n",
    "\n",
    "Поскольку логарифм положительного аргумента монотонно возрастает на всей области определения, то логарифмирование функции правдоподобия не изменит положение ее максимума. Значит, для удобства мы можем воспользоваться логарифмом функции правдоподобия:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\log L = \n",
    "\\log \\left(\\prod_{i=1}^{K}\\sigma_{i}(\\textbf{z})^{y_{i}}\\right) = \n",
    "\\sum_{i=1}^{K}y_{i}\\log \\sigma_{i}(\\textbf{z}) \\rightarrow max\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если домножить на $(-1)$, то получится выражение [кросс-энтропии](https://en.wikipedia.org/wiki/Cross_entropy) для многоклассовой классификации. Правдоподобие мы максимизируем, а кросс-энтропию, соответственно, минимизируем.\n",
    "\n",
    "$$\n",
    "H = \\left(-\\mathcal{L}\\right) \\rightarrow min\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого будем использовать методы градиентного спуска. Необходимо вывести выражение для компонент вектора градиента кросс-энтропии:\n",
    "\n",
    "$$\n",
    "\\dfrac {\\partial H} {\\partial w_{km}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим следующие частные производные:\n",
    "\n",
    "$$\n",
    "\\dfrac {\\partial H} {\\partial \\sigma_{k}} =\n",
    "\\dfrac {\\partial} {\\partial \\sigma_{k}} \\left( -\\sum_{i=1}^{K} y_i \\log \\sigma_i \\right) \n",
    "= - \\dfrac {y_k} {\\sigma_k}\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "$$\n",
    "\\dfrac {\\partial \\sigma_i} {\\partial z_k} = \n",
    "\\dfrac {\\partial} {\\partial z_k} \\left( \\dfrac {e^{z_i}} {\\sum_{j=1}^{K} e^{z_j}} \\right) =\n",
    "\\left \\{ \\begin{array}{lcl} \n",
    "\\dfrac {1} {\\left( \\sum_{j=1}^{K}e^{z_j} \\right)^2 } \\left( e^{z_k} \\sum_{j=1}^{K}e^{z_j} - e^{z_k} \\cdot e^{z_k} \\right) &=&\n",
    "\\sigma_k \\left( 1 - \\sigma_k \\right) && (i=k)\n",
    "\\\\ \n",
    "\\dfrac {1} {\\left( \\sum_{j=1}^{K}e^{z_j} \\right)^2 } \\left( -e^{z_i} \\cdot e^{z_k} \\right)\n",
    "&=& - \\sigma_i \\sigma_k && (i \\neq k)\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "$$\n",
    "\\dfrac {\\partial z_i} {\\partial w_{km}} =\n",
    "\\dfrac {\\partial} {\\partial w_{km}} \\left( \\sum_{j=1}^{M} w_{ij}x_j \\right) = \n",
    "\\left \\{ \\begin{array}{lcl} \n",
    "x_m && (i=k)\n",
    "\\\\\n",
    "0 && (i \\neq k)\n",
    "\\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем записать:\n",
    "\n",
    "$$\n",
    "\\dfrac {\\partial H} {\\partial z_k} = \n",
    "\\sum_{i=1}^{K} \\dfrac {\\partial H} {\\partial \\sigma_i} \\dfrac {\\partial \\sigma_i} {\\partial z_k} =\n",
    "\\dfrac {\\partial H} {\\partial \\sigma_k} \\dfrac {\\partial \\sigma_k} {\\partial z_k} +\n",
    "\\sum_{i \\neq k} \\dfrac {\\partial H} {\\partial \\sigma_i} \\dfrac {\\partial \\sigma_i} {\\partial z_k} =\n",
    "-y_k \\left( 1- \\sigma_k \\right) + \\sum_{i \\neq k} y_i \\sigma_k =\n",
    "-y_k + \\sigma_k \\sum_{i} y_i =\n",
    "\\sigma_k - y_k\n",
    "$$\n",
    "<br><br>\n",
    "$$\n",
    "\\dfrac {\\partial H} {\\partial w_{km}} =\n",
    "\\sum_{i=1}^{K} \\dfrac {\\partial H} {\\partial z_i} \\dfrac {\\partial z_i} {\\partial w_{km}} =\n",
    "x_m \\left( \\sigma_k - y_k \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача multilabel классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В другой постановке задачи каждый классифицируемый пример может иметь несколько тегов (принадлежать к нескольким классам). В этом случае требуется изменить модель:\n",
    "- будем считать, что все теги независимы друг от друга, и каждый исход - это логистическая регрессия на два класса (стратегия one-vs-all)\n",
    "- наличие каждого тега будем моделировать с помощью [распределения Бернулли](https://en.wikipedia.org/wiki/Bernoulli_distribution)\n",
    "\n",
    "Используя предыдущие выводы по логистической регрессии для двух классов, мы можем записать вероятность наличия тега (принадлежности к классу) следующим образом:\n",
    "\n",
    "$$\n",
    "p\\left( c=k \\mid \\textbf{x} \\right) = \\sigma \\left( z_k \\right) = \\sigma \\left( \\sum_{i=1}^{M} w_{ki}x_{i} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma(z_k) = \\dfrac {1} {1 - e^{-z_k}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_k = \\sum_{i=1}^Mw_{ki}x_i\n",
    "$$\n",
    "\n",
    "Отметим, что каждый тег (класс), как и для случая многоклассовой логрегрессии, имеет свой набор параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем функцию правдоподобия и ее логарифм:\n",
    "\n",
    "$$\n",
    "L \\left( \\theta \\mid \\textbf{x,y} \\right) = \\prod_{i=1}^{K} p_i^{y_i}\\left( 1-p_i \\right)^{1- y_i} =\n",
    "\\prod_{i=1}^{K} \\sigma(z_i)^{y_i}\\left( 1-\\sigma(z_i) \\right)^{1- y_i}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "$$\n",
    "\\mathcal{L} = \\log L = \\log \\left( \\prod_{i=1}^{K} \\sigma(z_i)^{y_i}\\left( 1-\\sigma(z_i) \\right)^{1- y_i} \\right) =\n",
    "\\sum_{i=1}^{K} \\left( \\log \\left( \\sigma(z_i)^{y_i}\\left( 1-\\sigma(z_i) \\right)^{1- y_i} \\right) \\right) =\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sum_{i=1}^{K} \\left( y_i \\log \\sigma \\left(z_i \\right) + \\left( 1-y_i \\right) \\log \\left( 1 - \\sigma \\left(z_i \\right) \\right) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И снова, после домножения на $(-1)$, мы получим выражение кросс-энтропии (в этот раз - для $K$ независимых классов). Нашей задачей стоит поиск ее минимума методами градиентного спуска, а для этого необходимо найти выражение для вычисления частных производных по параметрам модели:\n",
    "\n",
    "$$\n",
    "H = \\left(-\\mathcal{L}\\right) \\rightarrow min\n",
    "$$\n",
    "<br><br>\n",
    "$$\n",
    "\\dfrac {\\partial H} {\\partial w_{km}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим частные производные:\n",
    "\n",
    "$$\n",
    "\\dfrac {\\partial H} {\\partial \\sigma_k} =\n",
    "\\dfrac {\\partial} {\\partial \\sigma_k} \\left( (y_k - 1) \\log ( 1 - \\sigma_k ) - y_k \\log \\sigma_k \\right) =\n",
    "\\dfrac {1 - y_k} {1 - \\sigma_k} - \\dfrac {y_k} {\\sigma_k} =\n",
    "\\dfrac {\\sigma_k - y_k} {\\sigma_k (1 - \\sigma_k)}\n",
    "$$\n",
    "<br><br>\n",
    "\n",
    "$$\n",
    "\\dfrac {\\partial \\sigma_k} {\\partial z_k} =\n",
    "\\dfrac {\\partial} {\\partial z_k} \\left( \\dfrac {1} {1 - e^{-z_k}} \\right) = \n",
    "\\dfrac {e^{z_k} \\left( e^{z_k} - 1 \\right) - e^{z_k} \\cdot e^{z_k} } {\\left( e^{z_k} - 1 \\right)^2} =\n",
    "\\sigma_k - \\sigma_k^2 = \n",
    "\\sigma_k (1 - \\sigma_k)\n",
    "$$\n",
    "<br><br>\n",
    "\n",
    "$$\n",
    "\\dfrac {\\partial z_k} {\\partial w_{km}} =\n",
    "\\dfrac {\\partial} {\\partial w_{km}} \\left( \\sum_{i=1}^{M} w_{ki} x_i \\right) = x_m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем записать:\n",
    "\n",
    "$$\n",
    "\\dfrac {\\partial H} {\\partial w_{km}} =\n",
    "\\dfrac {\\partial H} {\\partial \\sigma_k} \\dfrac {\\partial \\sigma_k} {\\partial z_k} \n",
    "\\dfrac {\\partial z_k} {\\partial w_{km}} =\n",
    "x_m \\left( \\sigma_k - y_k \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для борьбы с переобучением модели будем использовать ElasticNet-регуляризацию. Это комбинация L1 (провоцирует на обнуление весов у малоинформативных признаков) и L2 (штрафует за большое по модулю значение весов) типов регуляризации. Запишем слагаемые в общей функции потерь, соответствующие L1 и L2 регуляризациям:\n",
    "\n",
    "$$\n",
    "L_1 = \\sum_{i=1}^{K} \\sum_{j=1}^{M} \\mid w_{ij} \\mid\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_2 = \\sum_{i=1}^{K} \\sum_{j=1}^{M} w_{ij}^2\n",
    "$$\n",
    "\n",
    "Введем параметры $\\lambda$ (коэффициент регуляризации) и $\\gamma$ (соотношение между двумя типами регуляризации). Тогда функция потерь примет вид:\n",
    "\n",
    "$$\n",
    "H_{reg} = H + \\lambda \\left( \\gamma \\sum_{i=1}^{K} \\sum_{j=1}^{M} w_{ij}^2 + \n",
    "(1 - \\gamma) \\sum_{i=1}^{K} \\sum_{j=1}^{M} \\mid w_{ij} \\mid \\right)\n",
    "$$\n",
    "\n",
    "Частная производная компонента регуляризации тогда запишется как:\n",
    "\n",
    "$$\n",
    "\\dfrac {\\partial} {\\partial w_{km}} \\left( \\lambda \\left( \\gamma \\sum_{i=1}^{K} \\sum_{j=1}^{M} w_{ij}^2 + \n",
    "(1 - \\gamma) \\sum_{i=1}^{K} \\sum_{j=1}^{M} \\mid w_{ij} \\mid \\right) \\right) =\n",
    "\\lambda \\left( 2 \\gamma w_{km} + (1 - \\gamma)sign(w_{km}) \\right)\n",
    "$$\n",
    "\n",
    "И, наконец, общий вид частной производной функции потерь с регуляризацией будет выглядеть как:\n",
    "\n",
    "$$\n",
    "\\dfrac {\\partial H} {\\partial w_{km}} = \n",
    "x_m \\left( \\sigma_k - y_k \\right) + \\lambda \\left( 2 \\gamma w_{km} + (1 - \\gamma)sign(w_{km}) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрика качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метрики качества классификации предлагается использовать [коэффициент Жаккара](https://en.wikipedia.org/wiki/Jaccard_index). Для конкретного объекта классификации коэффициент определяется как:\n",
    "\n",
    "$$\n",
    "J(T_p, T_t) = \\dfrac {\\mid T_p \\cap T_t \\mid} {\\mid T_p \\cup T_t \\mid}\n",
    "$$\n",
    "\n",
    "где $T_p$ - определенное моделью множество тегов объекта, а $T_е$ - фактическое множество тегов объекта. \n",
    "\n",
    "Метрика принимает значения на интервале $[0, 1]$, полное соответствие оценивается как $1$. Результирующую метрику на всей тестовой выборке будем определять как среднее по объектам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Файлы проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>olr_class.py</b>\n",
    "\n",
    "    Объявление и реализация класса модели.<br><br>\n",
    "    \n",
    "- <b>stackoverflow_scrapping.ipynb</b>\n",
    "\n",
    "    Notebook, выполняющий web scraping (парсинг данных) с сайта stackoverflow.com и формирующий датасет.<br><br>\n",
    "\n",
    "- <b>data_preprocess.ipynb</b>\n",
    "\n",
    "    Notebook, подготавливающий входные данные для модели.<br>\n",
    "    Сначала отбираются top-n тегов по всей выборке. Далее выборка фильтруется в соответствии с этими тегами (записи, не содержащие ни одного тега из top-n, не включаются в выборку; для остальных из всего списка тегов оставляются только входящие в top-n). После этого выборка разбивается в соответствии с заданной пропорцией на тренировочную и тестовую.Теги вынесены в отдельный файл.<br><br>\n",
    "    \n",
    "- <b>estimator_test.ipynb</b>\n",
    "\n",
    "    Notebook, содержащий тестирование класса модели.<br><br>\n",
    "    \n",
    "- <b>settings.json</b>\n",
    "\n",
    "    Найстройки проекта в формате json. Описания полей:\n",
    "    - data_dir - путь к каталогу данных\n",
    "    - data_file - имя файла датасета\n",
    "    - top_tags_count - количество top-n тегов, с которыми будет работать модель\n",
    "    - top_tags_file - имя создаваемого файла с top-n тегами\n",
    "    - filtered_tmp_file - имя создаваемого файла отфильтрованной выборки (в процессе удаляется)\n",
    "    - train_size - относительный размер тренировочного датасета\n",
    "    - train_file - имя создаваемого файла тренировочного датасета\n",
    "    - train_labels_file - имя создаваемого файла с тегами тренировочного датасета\n",
    "    - test_file - имя создаваемого файла тестового датасета\n",
    "    - test_labels_file - имя создаваемого файла с тегами тестового датасета\n",
    "    - additional_data_dir - путь к каталогу дополнительных данных (scraping)\n",
    "    - additional_data_file - шаблон имен файлов дополнительных датасетов\n",
    "    - objects_to_scrap - количество объектов, которые необходимо получить с сайта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для тестирования модели использовался датасет из 125 тысяч вопросов с stackoverflow.com с проставленными тегами. Выборка была разбита на обучающую и валидационную, соотношение - 0.8 (100 тысяч примеров для обучения и 25 тысяч - для валидации).<br><br>\n",
    "\n",
    "Тестирование модели проводилось по трем схемам обучения. После каждого прогона по обучающему датасету сохранялись значения функции потерь и оценки качества на обучающей и валидационной выборках. Далее строились графики - скользящее среднее функции потерь (окно в 20000 примеров) и метрики качества. Неоговоренные параметры модели оставлялись со значениями по умолчанию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>стратегия обучения - one-vs-rest, коэффициент регуляризации $\\lambda$=0.0002, порог предсказания класса - 0.5</b>. Выполняется первичный проход по датасету с формированием словаря, затем словарь фильтруется до топ-25000 популярных слов и выполянется еще один проход по датасету, затем фильтрация топ-10000 с еще одним проходом, и далее 2 прохода по обучающей выборке.\n",
    "<img src='./img/ovr_reg.png'>\n",
    "Можно наблюдать повышение качества классификации после каждой фильтрации словаря и последующим дообучением. Дополнительные проходы после фильтраций снижают качество классификации. Значения функции потерь для первого прохода максимальны, для 3-го и 4-го проходов имеют значительные всплески в районе середины обучающей выборки, чем отличаются по форме графика скользящего среднего от остальных проходов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>стратегия обучения - multinomial, коэффициент регуляризации $\\lambda$=0.0002, порог предсказания класса - 0.2</b>. Выполняется первичный проход по датасету с формированием словаря, затем словарь фильтруется до топ-25000 популярных слов и выполянется еще один проход по датасету, затем фильтрация топ-10000 с еще одним проходом, и далее 2 прохода по обучающей выборке.\n",
    "<img src='./img/mltnom_reg.png'>\n",
    "В целом графики скользящего среднего функций потерь имеют более шумный характер. Можно наблюдать повышение качества классификации после первой фильтрации словаря и дообучением после второй фильтрации. Последний проход снижает качество классификации. Значения функции потерь для первого прохода максимальны, для остальных проходов плотно лежат в одной области. Формы графиков для всех проходов идентичны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>стратегия обучения - one-vs-rest, коэффициент регуляризации $\\lambda$=0, порог предсказания класса - 0.8</b>. Выполняется первичный проход по датасету с формированием словаря, затем следует 4 прохода по обучающей выборке. Частотный словарь не сохраняется, что немнго увеличивает скорость первого прохода по датасету.\n",
    "<img src='./img/ovr_nonreg.png'>\n",
    "Графики функции потерь имеют самый сглаженный характер из всех схем обучения. Для обучающей выборки метрика качества росла с каждым проходом, для валидационной - на последнем проходе качество снизилось, что может говорить о переобучении (модель в данном случае не использовала регуляризацию). Значения функции потерь для первого прохода максимальны, для 3 и 4 проходов опять наблюдается всплеск потерь, в остальном формы графиков можно назвать идентичными. Модель с этими параметрами достигла максимальной оценки качества на валидационной выборке, но отличия незначительны относительно подобной модели, но с регуляризацией."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
