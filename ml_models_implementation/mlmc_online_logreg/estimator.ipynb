{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T19:27:16.431496Z",
     "start_time": "2018-08-02T19:27:16.419386Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<i><b>OnlineLogisticRegression(tags, strategy='ovr', learning_rate=0.1, lmbda=0.0002, gamma=0.1, tolerance=1e-16)</b></i><br>\n",
    "Конструктор класса.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Attributes:<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:20:56.041746Z",
     "start_time": "2018-08-02T17:20:55.988522Z"
    }
   },
   "outputs": [],
   "source": [
    "class OnlineLogisticRegression():\n",
    "    \n",
    "    \"\"\" OnlineLogisticRegression classifier\n",
    "    Класс реализует модель multiclass/multilabel классификации текстов, используя методы онлайн \n",
    "    обучения (стохастический градиентный спуск). В качестве регуляризатора используется ElasticNet \n",
    "    (комбинация L1 и L2). Поддерживаются повторные проходы по тренировочному датасету и ограничения \n",
    "    на размер словаря.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tags : [string]\n",
    "        Список допустимых для классификации тегов (классов). Не входящие в этот список теги \n",
    "        игнорируются.\n",
    "\n",
    "    strategy : ['ovr', 'multinomial'], default: 'ovr'\n",
    "        Признак типа классификации. Значение 'ovr' задает бинарную классификацию (присутствие/отсутствие) \n",
    "        для каждого тега, теги независимы (one-vs-rest, multilabel classification). Значение \n",
    "        'multinomial' задает минимизацию ошибки общего дискретного вероятностного распределения \n",
    "        (multiclass classification).\n",
    "    \n",
    "    learning_rate : float, default: 0.1\n",
    "        Скорость обучения градиентного спуска (множитель корректировки параметра модели на каждом шаге).\n",
    "\n",
    "    lmbda : float, default: 0.0002\n",
    "        Коэффициент ElasticNet-регуляризации на каждом шаге.\n",
    "\n",
    "    gamma : float, default: 0.1\n",
    "        Вес L2-компоненты в ElasticNet.\n",
    "\n",
    "    tolerance : float, default: 1e-16\n",
    "        Порог для огнаничения значений аргумента логарифма.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    vocab_ : dict {string: int}\n",
    "        Mapping слов-признаков в численные индексы. Слова добавляются в словарь в процессе обучения, \n",
    "        индексы назначаются инерементативно.\n",
    "\n",
    "    w_ : dict {string: defaultdict(int)}\n",
    "        Mapping тегов в словарь {<численный_индекс_признака>: <вес_в_модели>} (для каждого тега\n",
    "        свой набор параметров модели). Словарь изменяемого размера со значением по умолчанию 0.\n",
    "        \n",
    "    w0_ : dict {string: float}\n",
    "        Mapping тегов в веса w0 (смещения).\n",
    "        \n",
    "    train_frequency_dict_ : Counter\n",
    "        Counter-объект {<численный_индекс_признака>: <число_вхождений>}. Выполняет подсчет числа\n",
    "        вхождений признака на всей тренировочной выборке.\n",
    "    \"\"\"\n",
    "    def __init__(self, tags, strategy='ovr', learning_rate=0.1, lmbda=0.0002, gamma=0.1, tolerance=1e-16):\n",
    "        self.vocab_ = {}\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        self._train_frequency_dict = Counter()\n",
    "        \n",
    "        self.tags_ = set(tags)\n",
    "        self.strategy_ = strategy\n",
    "        self.learning_rate_ = learning_rate\n",
    "        self.lmbda_ = lmbda\n",
    "        self.gamma_ = gamma\n",
    "        self.tolerance_ = tolerance\n",
    "        \n",
    "    \n",
    "    \"\"\"Fit/update the model by passing the datasource\n",
    "    Обучение/дообучение модели одним проходом по источнику данных.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datasource : iterable\n",
    "        Итерируемый объект как источник данных. Формат примера из обучающей выборки - строка\n",
    "        с классифицируемым текстом и список тегов, разделенные знаком табуляции.\n",
    "        \n",
    "    total : int or None, default=None\n",
    "        Информация о количестве строк в источнике данных для вывода прогресс-бара. В случае\n",
    "        значения None, прогресс-бар не используется.\n",
    "        \n",
    "    update_vocab : bool, default=True\n",
    "        Флаг режима добавления слов в словарь (признаковое пространство) во время обучения.\n",
    "    \"\"\"\n",
    "    def fit(self, datasource, total=None, update_vocab=True):\n",
    "        self._loss = []\n",
    "        self._accuracy = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "                \n",
    "                # перечень предсказанных тегов для тестовой части выборки\n",
    "                predicted_tags = []\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "        \n",
    "                        if word not in self._vocab:\n",
    "                            # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                            if n >= top_n_train:\n",
    "                                continue\n",
    "                            # если встречаем новое слово с запретом на добавление, игнорируем его\n",
    "                            if not update_vocab:\n",
    "                                continue\n",
    "                            \n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        \n",
    "                        z += self._w[tag][self._vocab[word]] * 1.0\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    if z >= 0:\n",
    "                        sigma = 1 / (1 + np.exp(-z))\n",
    "                    else:\n",
    "                        sigma = 1 - 1 / (1 + np.exp(z))\n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    if y == 1:\n",
    "                        sample_loss += -1 * np.log(np.max([sigma, tolerance]))\n",
    "                    else:\n",
    "                        sample_loss += -1 * np.log(1 - np.min([1 - tolerance, sigma]))\n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        \n",
    "                        # учет xm будет реализовываться в цикле далее\n",
    "                        dLdw = (y - sigma)\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        regularized_words = set()\n",
    "                        \n",
    "                        for word in sentence: \n",
    "                            # игнорируем слово не из словаря\n",
    "                            if word not in self._vocab:\n",
    "                                continue\n",
    "                            \n",
    "                            # регуляризация веса только для первого вхождения слова\n",
    "                            if(self._vocab[word] not in regularized_words):\n",
    "                                w = self._w[tag][self._vocab[word]]\n",
    "                                regularization = lmbda * (2 * gamma * w + (1 - gamma) * np.sign(w))\n",
    "                                self._w[tag][self._vocab[word]] -= learning_rate * regularization\n",
    "                                regularized_words.add(self._vocab[word])\n",
    "                            \n",
    "                            # корректировка веса\n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                        \n",
    "                    # в тестовой части посчитаем прогноз    \n",
    "                    else:\n",
    "                        if sigma > predict_threshold:\n",
    "                            predicted_tags.append(tag)\n",
    "                \n",
    "                self._loss.append(sample_loss)\n",
    "                \n",
    "                # обновим частотный словарь\n",
    "                if n < top_n_train:\n",
    "                    if update_vocab:\n",
    "                        self._train_frequency_dict += Counter([self._vocab[word] for word in sentence])\n",
    "                        \n",
    "                # метрика качества для прогноза\n",
    "                else:\n",
    "                    k_jaccard = len(set(predicted_tags) & set(tags)) / len(set(predicted_tags) | set(tags))\n",
    "                    self._accuracy.append(k_jaccard)\n",
    "                    #print('tags: {} predicted: {} k_jaccard={}'.format(tags, predicted_tags, k_jaccard))\n",
    "                    \n",
    "                n += 1\n",
    "\n",
    "        return np.mean(self._accuracy)\n",
    "    \n",
    "    \"\"\"Отбор топ-n самых популярных слов в словаре (для всех тегов)\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    n : int, default=10000\n",
    "        количество слов для отбора\n",
    "    \"\"\"\n",
    "    def filter_vocab(self, n=10000):\n",
    "        \n",
    "        # мнлжество топ-n популярных слов\n",
    "        top_common_w = {k for (k, v) in self._train_frequency_dict.most_common(n)}\n",
    "\n",
    "        # обновим словарь\n",
    "        self._vocab = {key: val for (key, val) in self._vocab.items() if val in top_common_w}\n",
    "        \n",
    "        # обновим словари для тегов\n",
    "        for tag in self._tags:\n",
    "            self._w[tag] = {key: val for (key, val) in self._w[tag].items() if key in top_common_w}\n",
    "            \n",
    "    \"\"\"Предсказание тегов для вопроса\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    sentence : string\n",
    "        текст вопроса\n",
    "    \"\"\"\n",
    "    def predict_proba(self, sentence):        \n",
    "        \n",
    "        sentence = sentence.strip().split(' ')\n",
    "        predicted = dict()\n",
    "        \n",
    "        for tag in self._tags:\n",
    "            # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "            z = self._b[tag]\n",
    "            \n",
    "            for word in sentence:\n",
    "                if word not in self._vocab:\n",
    "                    continue\n",
    "                z += self._w[tag][self._vocab[word]] * 1.0\n",
    "\n",
    "            # вычисляем вероятность наличия тега\n",
    "            if z >= 0:\n",
    "                sigma = 1 / (1 + np.exp(-z))\n",
    "            else:\n",
    "                sigma = 1 - 1 / (1 + np.exp(z))\n",
    "\n",
    "            predicted[tag] = sigma\n",
    "        \n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
