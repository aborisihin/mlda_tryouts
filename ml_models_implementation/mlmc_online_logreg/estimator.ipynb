{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T05:06:27.996890Z",
     "start_time": "2018-08-03T05:06:27.228839Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:20:56.041746Z",
     "start_time": "2018-08-02T17:20:55.988522Z"
    }
   },
   "outputs": [],
   "source": [
    "class OnlineLogisticRegression():\n",
    "    \n",
    "    \"\"\" OnlineLogisticRegression classifier\n",
    "    Класс реализует модель multiclass/multilabel классификации текстов, используя методы онлайн \n",
    "    обучения (стохастический градиентный спуск). В качестве регуляризатора используется ElasticNet \n",
    "    (комбинация L1 и L2). Поддерживаются повторные проходы по тренировочному датасету и ограничения \n",
    "    на размер словаря.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tags : [string]\n",
    "        Список допустимых для классификации тегов (классов). Не входящие в этот список теги \n",
    "        игнорируются.\n",
    "\n",
    "    strategy : ['ovr', 'multinomial'], default: 'ovr'\n",
    "        Признак типа классификации. Значение 'ovr' задает бинарную классификацию (присутствие/отсутствие) \n",
    "        для каждого тега, теги независимы (one-vs-rest, multilabel classification). Значение \n",
    "        'multinomial' задает минимизацию ошибки общего дискретного вероятностного распределения \n",
    "        (multiclass classification).\n",
    "    \n",
    "    learning_rate : float, default: 0.1\n",
    "        Скорость обучения градиентного спуска (множитель корректировки параметра модели на каждом шаге).\n",
    "\n",
    "    lmbda : float, default: 0.0002\n",
    "        Коэффициент ElasticNet-регуляризации на каждом шаге.\n",
    "\n",
    "    gamma : float, default: 0.1\n",
    "        Вес L2-компоненты в ElasticNet.\n",
    "\n",
    "    tolerance : float, default: 1e-16\n",
    "        Порог для огнаничения значений аргумента логарифма.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    vocab_ : dict {string: int}\n",
    "        Mapping слов-признаков в численные индексы. Слова добавляются в словарь в процессе обучения, \n",
    "        индексы назначаются инкрементально.\n",
    "\n",
    "    w_ : dict {string: defaultdict(int)}\n",
    "        Mapping тегов в словарь {<численный_индекс_признака>: <вес_в_модели>} (для каждого тега\n",
    "        свой набор параметров модели). Словарь изменяемого размера со значением по умолчанию 0.\n",
    "        \n",
    "    w0_ : dict {string: float}\n",
    "        Mapping тегов в веса w0 (смещения).\n",
    "        \n",
    "    train_frequency_dict_ : Counter\n",
    "        Counter-объект {<численный_индекс_признака>: <число_вхождений>}. Выполняет подсчет числа\n",
    "        вхождений признака на всей тренировочной выборке.\n",
    "    \n",
    "    loss_ : [double]\n",
    "        Список значений функции потерь для последней используемой обучающей выборки.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, tags, strategy='ovr', learning_rate=0.1, lmbda=0.0002, gamma=0.1, tolerance=1e-16):\n",
    "        self.vocab_ = {}\n",
    "        self._w = {t: defaultdict(int) for t in tags}\n",
    "        self._b = {t: 0.0 for t in tags}\n",
    "        self._train_frequency_dict = Counter()\n",
    "        \n",
    "        self.tags_ = set(tags)\n",
    "        self.strategy_ = strategy\n",
    "        self.learning_rate_ = learning_rate\n",
    "        self.lmbda_ = lmbda\n",
    "        self.gamma_ = gamma\n",
    "        self.tolerance_ = tolerance\n",
    "        \n",
    "    \n",
    "    def fit(self, datasource, total=None, update_vocab=True, return_train_loss=False):\n",
    "        \"\"\"Fit/update the model by passing the datasource\n",
    "        Обучение/дообучение модели одним проходом по источнику данных.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        datasource : iterable\n",
    "            Итерируемый объект как источник данных. Формат примера из обучающей выборки - строка\n",
    "            с классифицируемым текстом и список тегов, разделенные знаком табуляции.\n",
    "\n",
    "        total : int or None, default=None\n",
    "            Информация о количестве строк в источнике данных для вывода прогресс-бара. В случае\n",
    "            значения None, прогресс-бар не используется.\n",
    "\n",
    "        update_vocab : bool, default=True\n",
    "            Флаг режима добавления слов в словарь (признаковое пространство) во время обучения.\n",
    "            \n",
    "        return_train_loss : bool, default=False\n",
    "            Флаг сохранения значений функции потерь для каждого примера из обучающей выборки.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Возвращает объект класса\n",
    "        \"\"\"\n",
    "        self.loss_ = [] \n",
    "        \n",
    "        if total is not None:\n",
    "            wrapped_source = tqdm_notebook(datasource, total=total, mininterval=1)\n",
    "        else:\n",
    "            wrapped_source = datasource\n",
    "            \n",
    "        for line in wrapped_source:\n",
    "            input_pair = line.strip().split('\\t')\n",
    "            if len(input_pair) != 2:\n",
    "                continue             \n",
    "                \n",
    "            word_sentence, tags = input_pair\n",
    "            word_sentence = word_sentence.split(' ')\n",
    "            tags = set(tags.split(' '))\n",
    "\n",
    "            # значение функции потерь для текущего примера\n",
    "            sample_loss = 0\n",
    "\n",
    "            # прокидываем градиенты для каждого тега\n",
    "            for tag in self._tags:\n",
    "                # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                y = int(tag in tags)\n",
    "\n",
    "                # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                # инициализируем z\n",
    "                z = self._b[tag]\n",
    "\n",
    "                for word in sentence:\n",
    "\n",
    "                    if word not in self._vocab:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train:\n",
    "                            continue\n",
    "                        # если встречаем новое слово с запретом на добавление, игнорируем его\n",
    "                        if not update_vocab:\n",
    "                            continue\n",
    "\n",
    "                        self._vocab[word] = len(self._vocab)\n",
    "\n",
    "                    z += self._w[tag][self._vocab[word]] * 1.0\n",
    "\n",
    "                # вычисляем вероятность наличия тега\n",
    "                if z >= 0:\n",
    "                    sigma = 1 / (1 + np.exp(-z))\n",
    "                else:\n",
    "                    sigma = 1 - 1 / (1 + np.exp(z))\n",
    "\n",
    "                # обновляем значение функции потерь для текущего примера\n",
    "                if y == 1:\n",
    "                    sample_loss += -1 * np.log(np.max([sigma, tolerance]))\n",
    "                else:\n",
    "                    sample_loss += -1 * np.log(1 - np.min([1 - tolerance, sigma]))\n",
    "\n",
    "                # если мы все еще в тренировочной части, то обновим параметры\n",
    "                if n < top_n_train:\n",
    "                    # вычисляем производную логарифмического правдоподобия по весу\n",
    "\n",
    "                    # учет xm будет реализовываться в цикле далее\n",
    "                    dLdw = (y - sigma)\n",
    "\n",
    "                    # делаем градиентный шаг\n",
    "                    # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                    # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                    regularized_words = set()\n",
    "\n",
    "                    for word in sentence: \n",
    "                        # игнорируем слово не из словаря\n",
    "                        if word not in self._vocab:\n",
    "                            continue\n",
    "\n",
    "                        # регуляризация веса только для первого вхождения слова\n",
    "                        if(self._vocab[word] not in regularized_words):\n",
    "                            w = self._w[tag][self._vocab[word]]\n",
    "                            regularization = lmbda * (2 * gamma * w + (1 - gamma) * np.sign(w))\n",
    "                            self._w[tag][self._vocab[word]] -= learning_rate * regularization\n",
    "                            regularized_words.add(self._vocab[word])\n",
    "\n",
    "                        # корректировка веса\n",
    "                        self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "\n",
    "                    self._b[tag] -= -learning_rate*dLdw\n",
    "\n",
    "                # в тестовой части посчитаем прогноз    \n",
    "                else:\n",
    "                    if sigma > predict_threshold:\n",
    "                        predicted_tags.append(tag)\n",
    "\n",
    "            self._loss.append(sample_loss)\n",
    "\n",
    "            # обновим частотный словарь\n",
    "            if n < top_n_train:\n",
    "                if update_vocab:\n",
    "                    self._train_frequency_dict += Counter([self._vocab[word] for word in sentence])\n",
    "\n",
    "            # метрика качества для прогноза\n",
    "            else:\n",
    "                k_jaccard = len(set(predicted_tags) & set(tags)) / len(set(predicted_tags) | set(tags))\n",
    "                self._accuracy.append(k_jaccard)\n",
    "                #print('tags: {} predicted: {} k_jaccard={}'.format(tags, predicted_tags, k_jaccard))\n",
    "                \n",
    "        return self\n",
    "\n",
    "    \n",
    "    \"\"\"Отбор топ-n самых популярных слов в словаре (для всех тегов)\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    n : int, default=10000\n",
    "        количество слов для отбора\n",
    "    \"\"\"\n",
    "    def filter_vocab(self, n=10000):\n",
    "        \n",
    "        # мнлжество топ-n популярных слов\n",
    "        top_common_w = {k for (k, v) in self._train_frequency_dict.most_common(n)}\n",
    "\n",
    "        # обновим словарь\n",
    "        self._vocab = {key: val for (key, val) in self._vocab.items() if val in top_common_w}\n",
    "        \n",
    "        # обновим словари для тегов\n",
    "        for tag in self._tags:\n",
    "            self._w[tag] = {key: val for (key, val) in self._w[tag].items() if key in top_common_w}\n",
    "            \n",
    "    \"\"\"Предсказание тегов для вопроса\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    sentence : string\n",
    "        текст вопроса\n",
    "    \"\"\"\n",
    "    def predict_proba(self, sentence):        \n",
    "        \n",
    "        sentence = sentence.strip().split(' ')\n",
    "        predicted = dict()\n",
    "        \n",
    "        for tag in self._tags:\n",
    "            # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "            z = self._b[tag]\n",
    "            \n",
    "            for word in sentence:\n",
    "                if word not in self._vocab:\n",
    "                    continue\n",
    "                z += self._w[tag][self._vocab[word]] * 1.0\n",
    "\n",
    "            # вычисляем вероятность наличия тега\n",
    "            if z >= 0:\n",
    "                sigma = 1 / (1 + np.exp(-z))\n",
    "            else:\n",
    "                sigma = 1 - 1 / (1 + np.exp(z))\n",
    "\n",
    "            predicted[tag] = sigma\n",
    "        \n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
