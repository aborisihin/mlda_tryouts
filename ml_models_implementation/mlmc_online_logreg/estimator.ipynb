{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T05:06:27.996890Z",
     "start_time": "2018-08-03T05:06:27.228839Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:20:56.041746Z",
     "start_time": "2018-08-02T17:20:55.988522Z"
    }
   },
   "outputs": [],
   "source": [
    "class OnlineLogisticRegression():\n",
    "    \n",
    "    \"\"\" OnlineLogisticRegression classifier\n",
    "    Класс реализует модель multiclass/multilabel классификации текстов, используя методы онлайн \n",
    "    обучения (стохастический градиентный спуск). В качестве регуляризатора используется ElasticNet \n",
    "    (комбинация L1 и L2). Поддерживаются повторные проходы по тренировочному датасету и ограничения \n",
    "    на размер словаря.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tags : [string]\n",
    "        Список допустимых для классификации тегов (классов). Не входящие в этот список теги \n",
    "        игнорируются.\n",
    "\n",
    "    strategy : ['ovr', 'multinomial'], default: 'ovr'\n",
    "        Признак типа классификации. Значение 'ovr' задает бинарную классификацию (присутствие/отсутствие) \n",
    "        для каждого тега, теги независимы (one-vs-rest, multilabel classification). Значение \n",
    "        'multinomial' задает минимизацию ошибки общего дискретного вероятностного распределения \n",
    "        (multiclass classification).\n",
    "    \n",
    "    learning_rate : float, default: 0.1\n",
    "        Скорость обучения градиентного спуска (множитель корректировки параметра модели на каждом шаге).\n",
    "\n",
    "    lmbda : float, default: 0.0002\n",
    "        Коэффициент ElasticNet-регуляризации на каждом шаге.\n",
    "\n",
    "    gamma : float, default: 0.1\n",
    "        Вес L2-компоненты в ElasticNet.\n",
    "\n",
    "    tolerance : float, default: 1e-16\n",
    "        Порог для огнаничения значений аргумента логарифма.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    vocab_ : dict {string: int}\n",
    "        Mapping слов-признаков в численные индексы. Слова добавляются в словарь в процессе обучения, \n",
    "        индексы назначаются инкрементально.\n",
    "        Т.к. обучение модели ведется по онлайн-схеме, то всего признакового пространства мы не знаем.\n",
    "        В этом случае пользоваться bag-of-words или, например, CountVectorizer из sklearn, не \n",
    "        целесообразно (словарь придется пересчитывать при каждом появлении нового слова).\n",
    "        Соответственно, на каждой итерации обучения линейная комбинация весов и признаков z рассчитывается\n",
    "        как сумма весов модели для каждого встреченного слова (если слово втречалось несколько раз, то и в \n",
    "        итоговую сумму его вес войдет такое же число раз; для остальных слов из словаря вхождений не будет).\n",
    "\n",
    "    w_ : dict {string: defaultdict(int)}\n",
    "        Mapping тегов в словарь {<численный_индекс_признака>: <вес_в_модели>} (для каждого тега\n",
    "        свой набор параметров модели). Словарь изменяемого размера со значением по умолчанию 0.\n",
    "        \n",
    "    w0_ : dict {string: float}\n",
    "        Mapping тегов в веса w0 (смещения).\n",
    "        \n",
    "    train_frequency_dict_ : Counter\n",
    "        Counter-объект {<численный_индекс_признака>: <число_вхождений>}. Выполняет подсчет числа\n",
    "        вхождений признака на всей тренировочной выборке.\n",
    "    \n",
    "    loss_ : [double]\n",
    "        Список значений функции потерь для последней используемой обучающей выборки.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, tags, strategy='ovr', learning_rate=0.1, lmbda=0.0002, gamma=0.1, tolerance=1e-16):\n",
    "        self.vocab_ = {}\n",
    "        self._w = {t: defaultdict(int) for t in tags}\n",
    "        self._b = {t: 0.0 for t in tags}\n",
    "        self._train_frequency_dict = Counter()\n",
    "        \n",
    "        self.tags_ = set(tags)\n",
    "        self.strategy_ = strategy\n",
    "        self.learning_rate_ = learning_rate\n",
    "        self.lmbda_ = lmbda\n",
    "        self.gamma_ = gamma\n",
    "        self.tolerance_ = tolerance\n",
    "        \n",
    "    \n",
    "    def fit(self, datasource, total=None, update_vocab=True, return_train_loss=False):\n",
    "        \"\"\"Fit/update the model by passing the datasource\n",
    "        Обучение/дообучение модели одним проходом по источнику данных.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        datasource : iterable\n",
    "            Итерируемый объект как источник данных. Формат примера из обучающей выборки - строка\n",
    "            с классифицируемым текстом и список тегов, разделенные знаком табуляции.\n",
    "\n",
    "        total : int or None, default=None\n",
    "            Информация о количестве строк в источнике данных для вывода прогресс-бара. В случае\n",
    "            значения None, прогресс-бар не используется.\n",
    "\n",
    "        update_vocab : bool, default=True\n",
    "            Флаг режима добавления слов в словарь (признаковое пространство) во время обучения.\n",
    "            \n",
    "        return_train_loss : bool, default=False\n",
    "            Флаг сохранения значений функции потерь для каждого примера из обучающей выборки.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Возвращает объект класса\n",
    "        \"\"\"\n",
    "        self.loss_ = [] \n",
    "        \n",
    "        if total is not None:\n",
    "            wrapped_source = tqdm_notebook(datasource, total=total, mininterval=1)\n",
    "        else:\n",
    "            wrapped_source = datasource\n",
    "            \n",
    "        for line in wrapped_source:\n",
    "            \n",
    "            input_pair = line.strip().split('\\t')\n",
    "            if len(input_pair) != 2:\n",
    "                continue             \n",
    "                \n",
    "            word_sentence = input_pair[0].split(' ')\n",
    "            sample_tags = set(input_pair[1].split(' '))\n",
    "            \n",
    "            # отбор только известных тегов\n",
    "            sample_tags = sample_tags & self.tags_ \n",
    "            \n",
    "            if len(sample_tags) == 0:\n",
    "                continue\n",
    "\n",
    "            # значение функции потерь для текущего примера\n",
    "            sample_loss = 0\n",
    "\n",
    "            # градиентный спуск для каждого тега\n",
    "            for tag in self.tags_:\n",
    "                \n",
    "                # целевая переменная\n",
    "                y = int(tag in tags)\n",
    "\n",
    "                # инициализируем z (линейная комбинация весов и признаков объекта) смещением\n",
    "                z = self.w0_[tag]\n",
    "\n",
    "                for word in sentence:\n",
    "                    \n",
    "                    # обработка слова не из словаря\n",
    "                    if word not in self.vocab_:\n",
    "                        if update_vocab:\n",
    "                            self.vocab_[word] = len(self.vocab_)\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    z += self.w_[tag][self.vocab_[word]]\n",
    "\n",
    "                # вычисляем сигмоид (фактически, это вероятность наличия тега)\n",
    "                # чтобы не столкнуться с overflow, избегаем вычисления экспоненты \n",
    "                # с очень большим по модулю положительным аргументом\n",
    "                if z >= 0:\n",
    "                    sigma = 1 / (1 + np.exp(-z))\n",
    "                else:\n",
    "                    sigma = 1 - 1 / (1 + np.exp(z))\n",
    "\n",
    "                # обновляем значение функции потерь для текущего примера\n",
    "                # чтобы не получить потери точности, избегаем вычисления логарифма с\n",
    "                # близким к 0 или 1 аргументом, используя порог tolerance\n",
    "                if y == 1:\n",
    "                    sample_loss += -1 * np.log(np.max([sigma, tolerance]))\n",
    "                else:\n",
    "                    sample_loss += -1 * np.log(1 - np.min([1 - tolerance, sigma]))\n",
    "\n",
    "                # обновим параметры модели\n",
    "                \n",
    "                # вычисляем частную производную функции потерь по текущему весу\n",
    "                # учет xm будет реализовываться в цикле далее\n",
    "                dHdw = (sigma - y)\n",
    "\n",
    "                # делаем градиентный шаг и выполняем регуляризацию\n",
    "                # в целях увеличения производительности делаем допущение для регуляризации\n",
    "                # чтобы в каждой итерации обучения не выполнять регуляризацию всех параметров,\n",
    "                # будем учитывать только присутствующие в текущем обучающем примере признаки;\n",
    "                # естественно, каждый признак должен быть регуляризован только один раз, не\n",
    "                # учитывая число его вхождений в обучающий пример\n",
    "                # будем выполнять регуляризацию во время первого появления признака\n",
    "                \n",
    "                regularized_words = set()\n",
    "\n",
    "                for word in sentence: \n",
    "                    \n",
    "                    # игнорируем слово не из словаря\n",
    "                    if word not in self._vocab:\n",
    "                        continue\n",
    "\n",
    "                    # регуляризация веса только для первого вхождения слова\n",
    "                    regularization = 0.0\n",
    "                    \n",
    "                    if(self._vocab[word] not in regularized_words):\n",
    "                        w = self._w[tag][self._vocab[word]]\n",
    "                        regularization = lmbda * (2 * gamma * w + (1 - gamma) * np.sign(w))\n",
    "                        regularized_words.add(self._vocab[word])\n",
    "\n",
    "                    # корректировка веса\n",
    "                    # явное указание множителя 1.0 показывает, что мы не забыли множитель xm\n",
    "                    self._w[tag][self._vocab[word]] -= learning_rate * (1.0 * dHdw + regularization)\n",
    "\n",
    "                    # смещение не регуляризируется\n",
    "                    self._b[tag] -= learning_rate * 1.0 * dLdw\n",
    "\n",
    "            self._loss.append(sample_loss)\n",
    "\n",
    "            # обновим частотный словарь\n",
    "            if update_vocab:\n",
    "                self._train_frequency_dict += Counter([self._vocab[word] for word in sentence])\n",
    "                \n",
    "        return self\n",
    "\n",
    "    \n",
    "    \"\"\"Отбор топ-n самых популярных слов в словаре (для всех тегов)\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    n : int, default=10000\n",
    "        количество слов для отбора\n",
    "    \"\"\"\n",
    "    def filter_vocab(self, n=10000):\n",
    "        \n",
    "        # мнлжество топ-n популярных слов\n",
    "        top_common_w = {k for (k, v) in self._train_frequency_dict.most_common(n)}\n",
    "\n",
    "        # обновим словарь\n",
    "        self._vocab = {key: val for (key, val) in self._vocab.items() if val in top_common_w}\n",
    "        \n",
    "        # обновим словари для тегов\n",
    "        for tag in self._tags:\n",
    "            self._w[tag] = {key: val for (key, val) in self._w[tag].items() if key in top_common_w}\n",
    "            \n",
    "    \"\"\"Предсказание тегов для вопроса\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    sentence : string\n",
    "        текст вопроса\n",
    "    \"\"\"\n",
    "    def predict_proba(self, sentence):        \n",
    "        \n",
    "        sentence = sentence.strip().split(' ')\n",
    "        predicted = dict()\n",
    "        \n",
    "        for tag in self._tags:\n",
    "            # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "            z = self._b[tag]\n",
    "            \n",
    "            for word in sentence:\n",
    "                if word not in self._vocab:\n",
    "                    continue\n",
    "                z += self._w[tag][self._vocab[word]] * 1.0\n",
    "\n",
    "            # вычисляем вероятность наличия тега\n",
    "            if z >= 0:\n",
    "                sigma = 1 / (1 + np.exp(-z))\n",
    "            else:\n",
    "                sigma = 1 - 1 / (1 + np.exp(z))\n",
    "\n",
    "            predicted[tag] = sigma\n",
    "        \n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
