{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T05:52:09.196859Z",
     "start_time": "2018-08-06T05:52:09.189623Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T05:52:09.693174Z",
     "start_time": "2018-08-06T05:52:09.664165Z"
    }
   },
   "outputs": [],
   "source": [
    "class OnlineLogisticRegression():\n",
    "    \n",
    "    \"\"\" OnlineLogisticRegression classifier\n",
    "    Класс реализует модель multiclass/multilabel классификации текстов, используя методы онлайн \n",
    "    обучения (стохастический градиентный спуск). В качестве регуляризатора используется ElasticNet \n",
    "    (комбинация L1 и L2). Поддерживаются повторные проходы по тренировочному датасету и ограничения \n",
    "    на размер словаря.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tags : [string]\n",
    "        Список допустимых для классификации тегов (классов). Не входящие в этот список теги \n",
    "        игнорируются.\n",
    "\n",
    "    strategy : ['ovr', 'multinomial'], default: 'ovr'\n",
    "        Признак типа классификации. Значение 'ovr' задает бинарную классификацию (присутствие/отсутствие) \n",
    "        для каждого тега, теги независимы (one-vs-rest, multilabel classification). Значение \n",
    "        'multinomial' задает минимизацию ошибки общего дискретного вероятностного распределения \n",
    "        (multiclass classification).\n",
    "    \n",
    "    learning_rate : float, default: 0.1\n",
    "        Скорость обучения градиентного спуска (множитель корректировки параметра модели на каждом шаге).\n",
    "\n",
    "    lmbda : float, default: 0.0002\n",
    "        Коэффициент ElasticNet-регуляризации на каждом шаге.\n",
    "\n",
    "    gamma : float, default: 0.1\n",
    "        Вес L2-компоненты в ElasticNet.\n",
    "        \n",
    "    store_frequency : bool, default: True\n",
    "        Флаг хранения частот слов в выборке для последующего снижения размерности признакового \n",
    "        пространства (сильно понижающая скорость первоначального обучения операция).\n",
    "\n",
    "    tolerance : float, default: 1e-16\n",
    "        Порог для огнаничения значений аргумента логарифма.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    vocab_ : dict {string: int}\n",
    "        Mapping слов-признаков в численные индексы. Слова добавляются в словарь в процессе обучения, \n",
    "        индексы назначаются инкрементально.\n",
    "        Т.к. обучение модели ведется по онлайн-схеме, то всего признакового пространства мы не знаем.\n",
    "        В этом случае пользоваться bag-of-words или, например, CountVectorizer из sklearn, не \n",
    "        целесообразно (словарь придется пересчитывать при каждом появлении нового слова).\n",
    "\n",
    "    w_ : dict {string: defaultdict(int)}\n",
    "        Mapping тегов в словарь {<численный_индекс_признака>: <вес_в_модели>} (для каждого тега\n",
    "        свой набор параметров модели). Словарь изменяемого размера со значением по умолчанию 0.\n",
    "        \n",
    "    w0_ : dict {string: float}\n",
    "        Mapping тегов в веса w0 (смещения).\n",
    "        \n",
    "    train_frequency_dict_ : Counter\n",
    "        Counter-объект {<численный_индекс_признака>: <число_вхождений>}. Выполняет подсчет числа\n",
    "        вхождений признака на всей тренировочной выборке.\n",
    "    \n",
    "    loss_ : [double]\n",
    "        Список значений функции потерь для последней используемой обучающей выборки.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, tags, strategy='ovr', learning_rate=0.1, lmbda=0.0002, gamma=0.1, \n",
    "                 store_frequency=True, tolerance=1e-16):\n",
    "        self.vocab_ = {}\n",
    "        self.w_ = {t: defaultdict(int) for t in tags}\n",
    "        self.w0_ = {t: 0.0 for t in tags}\n",
    "        self.train_frequency_dict_ = Counter()\n",
    "        self.loss_ = []\n",
    "        \n",
    "        self.tags_ = set(tags)\n",
    "        self.strategy_ = strategy\n",
    "        self.learning_rate_ = learning_rate\n",
    "        self.lmbda_ = lmbda\n",
    "        self.gamma_ = gamma\n",
    "        self.store_frequency_ = store_frequency\n",
    "        self.tolerance_ = tolerance\n",
    "        \n",
    "    \n",
    "    def fit(self, datasource, total=None, update_vocab=True, return_train_loss=False):\n",
    "        \"\"\"Fit/update the model by passing the datasource\n",
    "        Обучение/дообучение модели одним проходом по источнику данных.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        datasource : iterable\n",
    "            Итерируемый объект как источник данных. Формат примера из обучающей выборки - строка\n",
    "            с классифицируемым текстом и список тегов, разделенные знаком табуляции.\n",
    "\n",
    "        total : int or None, default=None\n",
    "            Информация о количестве строк в источнике данных для вывода прогресс-бара. В случае\n",
    "            значения None, прогресс-бар не используется.\n",
    "\n",
    "        update_vocab : bool, default=True\n",
    "            Флаг режима добавления слов в словарь (признаковое пространство) во время обучения.\n",
    "            \n",
    "        return_train_loss : bool, default=False\n",
    "            Флаг сохранения значений функции потерь для каждого примера из обучающей выборки.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Возвращает объект класса\n",
    "        \"\"\"\n",
    "        self.loss_ = [] \n",
    "        \n",
    "        if total is not None:\n",
    "            wrapped_source = tqdm_notebook(datasource, total=total, mininterval=1)\n",
    "        else:\n",
    "            wrapped_source = datasource\n",
    "            \n",
    "        for line in wrapped_source:\n",
    "            \n",
    "            input_pair = line.strip().split('\\t')\n",
    "            if len(input_pair) != 2:\n",
    "                continue             \n",
    "                \n",
    "            word_sentence = input_pair[0].split(' ')\n",
    "            sample_tags = set(input_pair[1].split(' '))\n",
    "            \n",
    "            # отбор только известных тегов\n",
    "            sample_tags = sample_tags & self.tags_ \n",
    "            \n",
    "            if len(sample_tags) == 0:\n",
    "                continue\n",
    "\n",
    "            # значение функции потерь для текущего примера\n",
    "            sample_loss = 0\n",
    "\n",
    "            # градиентный спуск для каждого тега\n",
    "            for tag in self.tags_:\n",
    "                \n",
    "                # целевая переменная\n",
    "                y = int(tag in sample_tags)\n",
    "\n",
    "                # инициализируем z (линейная комбинация весов и признаков объекта) смещением\n",
    "                z = self.w0_[tag]\n",
    "\n",
    "                # чтобы не пробегать словарь на каждой строчке в процессе обучения, линейная \n",
    "                # комбинация весов и признаков z рассчитывается как сумма весов модели для \n",
    "                # каждого встреченного слова (если слово втречалось несколько раз, то и в \n",
    "                # итоговую сумму его вес войдет такое же число раз; для остальных слов из \n",
    "                # словаря вхождений не будет)\n",
    "                for word in word_sentence:\n",
    "                    \n",
    "                    # обработка слова не из словаря\n",
    "                    if word not in self.vocab_:\n",
    "                        if update_vocab:\n",
    "                            self.vocab_[word] = len(self.vocab_)\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    z += self.w_[tag][self.vocab_[word]]\n",
    "\n",
    "                # вычисляем сигмоид (фактически, это вероятность наличия тега);\n",
    "                # чтобы не столкнуться с overflow, избегаем вычисления экспоненты \n",
    "                # с очень большим по модулю положительным аргументом\n",
    "                if z >= 0:\n",
    "                    sigma = 1 / (1 + np.exp(-z))\n",
    "                else:\n",
    "                    sigma = 1 - 1 / (1 + np.exp(z))\n",
    "\n",
    "                # обновляем значение функции потерь для текущего примера;\n",
    "                # чтобы не получить потери точности, избегаем вычисления логарифма с\n",
    "                # близким к 0 или 1 аргументом, используя порог tolerance\n",
    "                if y == 1:\n",
    "                    sample_loss += -1 * np.log(np.max([sigma, self.tolerance_]))\n",
    "                else:\n",
    "                    sample_loss += -1 * np.log(1 - np.min([1 - self.tolerance_, sigma]))\n",
    "\n",
    "                # обновим параметры модели\n",
    "                \n",
    "                # вычисляем частную производную функции потерь по текущему весу;\n",
    "                # учет xm будет реализовываться в цикле далее\n",
    "                dHdw = (sigma - y)\n",
    "\n",
    "                # делаем градиентный шаг и выполняем регуляризацию;\n",
    "                # в целях увеличения производительности делаем допущение для регуляризации;\n",
    "                # чтобы в каждой итерации обучения не выполнять регуляризацию всех параметров,\n",
    "                # будем учитывать только присутствующие в текущем обучающем примере признаки;\n",
    "                # естественно, каждый признак должен быть регуляризован только один раз, не\n",
    "                # учитывая число его вхождений в обучающий пример;\n",
    "                # будем выполнять регуляризацию во время первого появления признака\n",
    "                \n",
    "                regularized_words = set()\n",
    "\n",
    "                for word in word_sentence: \n",
    "                    \n",
    "                    if word not in self.vocab_:\n",
    "                        continue\n",
    "\n",
    "                    regularization = 0.0\n",
    "                    \n",
    "                    if self.lmbda_ > 0.0:\n",
    "                        if self.vocab_[word] not in regularized_words:                    \n",
    "                            regularized_words.add(self.vocab_[word])\n",
    "                            w = self.w_[tag][self.vocab_[word]]    \n",
    "                            regularization = self.lmbda_ * (2 * self.gamma_ * w + (1 - self.gamma_) * np.sign(w))\n",
    "\n",
    "                    # корректировка веса значением антиградиента;\n",
    "                    # явное указание 1.0 показывает, что мы не забыли множитель xm\n",
    "                    self.w_[tag][self.vocab_[word]] -= self.learning_rate_ * (1.0 * dHdw + regularization)\n",
    "\n",
    "                # смещение не регуляризируется\n",
    "                self.w0_[tag] -= self.learning_rate_ * 1.0 * dHdw\n",
    "\n",
    "            self.loss_.append(sample_loss)\n",
    "\n",
    "            # обновим частотный словарь\n",
    "            if self.store_frequency_ & update_vocab:\n",
    "                self.train_frequency_dict_ += Counter([self.vocab_[word] for word in word_sentence])\n",
    "                \n",
    "        return self\n",
    "\n",
    "\n",
    "    def filter_vocab(self, n=10000):\n",
    "        \"\"\" Filtering vocabulary by the top-n words (for all classes)\n",
    "        Отбор топ-n самых популярных слов в словаре (для всех тегов)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int, default=10000\n",
    "            количество слов для отбора\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Возвращает объект класса\n",
    "        \"\"\"\n",
    "        if not self.store_frequency_:\n",
    "            print('can\\'t filter vocabulary case no frequency data')\n",
    "            return\n",
    "        \n",
    "        top_words = {k for (k, v) in self.train_frequency_dict_.most_common(n)}\n",
    "\n",
    "        # обновим словарь\n",
    "        self.vocab_ = {key: val for (key, val) in self.vocab_.items() if val in top_words}\n",
    "        \n",
    "        # обновим словари весов для тегов\n",
    "        for tag in self.tags_:\n",
    "            self.w_[tag] = {key: val for (key, val) in self.w_[tag].items() if key in top_words}\n",
    "            \n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict_proba(self, datasource, total=None):        \n",
    "        \"\"\" \n",
    "        Предсказание тегов для текста\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        datasource : iterable\n",
    "            Итерируемый объект как источник данных, содержащий строки для классификации\n",
    "            \n",
    "        total : int or None, default=None\n",
    "            Информация о количестве строк в источнике данных для вывода прогресс-бара. В случае\n",
    "            значения None, прогресс-бар не используется.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        predicted : list of [(tag, probability), ...]\n",
    "            Возвращает список, где элементом является список кортежей вида \n",
    "            (<тег>, <вероятность>)\n",
    "        \"\"\"\n",
    "        predicted = []\n",
    "        \n",
    "        if total is not None:\n",
    "            wrapped_source = tqdm_notebook(datasource, total=total, mininterval=1)\n",
    "        else:\n",
    "            wrapped_source = datasource\n",
    "            \n",
    "        for line in wrapped_source:\n",
    "        \n",
    "            sentence = line.strip().split(' ')\n",
    "            line_predicted = dict()\n",
    "\n",
    "            for tag in self.tags_:\n",
    "                # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                z = self.w0_[tag]\n",
    "\n",
    "                for word in sentence:\n",
    "                    if word not in self.vocab_:\n",
    "                        continue\n",
    "                    z += self.w_[tag][self.vocab_[word]] * 1.0\n",
    "\n",
    "                # вычисляем вероятность наличия тега\n",
    "                if z >= 0:\n",
    "                    sigma = 1 / (1 + np.exp(-z))\n",
    "                else:\n",
    "                    sigma = 1 - 1 / (1 + np.exp(z))\n",
    "\n",
    "                line_predicted[tag] = sigma\n",
    "                \n",
    "            predicted.append(line_predicted)\n",
    "        \n",
    "        return pd.DataFrame(predicted)\n",
    "    \n",
    "    \n",
    "    def score(self, datasource, labels_datasource):\n",
    "        \"\"\" Returns the mean Jaccard index on the given dataset and labels\n",
    "        Возвращает метрику качества (средний коэффициент Жаккара) по выборке с \n",
    "        указанными метками\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : iterable\n",
    "            Итерируемый объект как источник данных, содержащий строки для классификации\n",
    "            \n",
    "        total : int or None, default=None\n",
    "            Информация о количестве строк в источнике данных для вывода прогресс-бара. В случае\n",
    "            значения None, прогресс-бар не используется.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        predicted : list of [(tag, probability), ...]\n",
    "            Возвращает список, где элементом является список кортежей вида \n",
    "            (<тег>, <вероятность>)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T05:52:10.519565Z",
     "start_time": "2018-08-06T05:52:10.513191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys in settings file:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data_dir',\n",
       " 'data_file',\n",
       " 'top_tags_count',\n",
       " 'top_tags_file',\n",
       " 'filtered_tmp_file',\n",
       " 'train_size',\n",
       " 'train_file',\n",
       " 'test_file',\n",
       " 'test_labels_file']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загрузка настроек\n",
    "with open('./settings.json', 'r') as settings_file:\n",
    "    settings = json.load(settings_file)\n",
    "\n",
    "print('keys in settings file:')\n",
    "list(settings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T05:52:11.363635Z",
     "start_time": "2018-08-06T05:52:11.360411Z"
    }
   },
   "outputs": [],
   "source": [
    "top_tags_filepath = os.path.join(settings['data_dir'], settings['top_tags_file'])\n",
    "train_filepath = os.path.join(settings['data_dir'], settings['train_file'])\n",
    "test_filepath = os.path.join(settings['data_dir'], settings['test_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T05:52:11.988950Z",
     "start_time": "2018-08-06T05:52:11.971731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-10 tags:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['javascript',\n",
       " 'java',\n",
       " 'c#',\n",
       " 'php',\n",
       " 'android',\n",
       " 'jquery',\n",
       " 'python',\n",
       " 'html',\n",
       " 'c++',\n",
       " 'ios']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загрузка топ тегов\n",
    "top_tags_dataframe = pd.read_csv(top_tags_filepath, header=None)\n",
    "top_tags = top_tags_dataframe[0].tolist()\n",
    "\n",
    "print('top-{} tags:'.format(settings['top_tags_count']))\n",
    "top_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T05:52:13.105737Z",
     "start_time": "2018-08-06T05:52:12.921692Z"
    }
   },
   "outputs": [],
   "source": [
    "# размер тренировочного датасета\n",
    "train_lines_count = int(subprocess.check_output(['wc', '-l', train_filepath]).split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T05:52:13.947629Z",
     "start_time": "2018-08-06T05:52:13.944439Z"
    }
   },
   "outputs": [],
   "source": [
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T05:52:18.300492Z",
     "start_time": "2018-08-06T05:52:18.297319Z"
    }
   },
   "outputs": [],
   "source": [
    "model_ss = OnlineLogisticRegression(top_tags, lmbda=0.0, store_frequency=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T05:52:52.461177Z",
     "start_time": "2018-08-06T05:52:18.980274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07f942ee62b4280a69362a41fa6e62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12501), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(train_filepath) as train_file:\n",
    "    model_ss.fit(train_file, total=train_lines_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T05:54:32.809580Z",
     "start_time": "2018-08-06T05:52:52.463085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6688a57daf45bfbb877c607b3a3bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=112499), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_lines_count = int(subprocess.check_output(['wc', '-l', test_filepath]).split()[0])\n",
    "\n",
    "with open(test_filepath) as test_file:\n",
    "    predicted = model_ss.predict_proba(test_file, total=test_lines_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T05:54:32.834223Z",
     "start_time": "2018-08-06T05:54:32.812073Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>android</th>\n",
       "      <th>c#</th>\n",
       "      <th>c++</th>\n",
       "      <th>html</th>\n",
       "      <th>ios</th>\n",
       "      <th>java</th>\n",
       "      <th>javascript</th>\n",
       "      <th>jquery</th>\n",
       "      <th>php</th>\n",
       "      <th>python</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.461698e-11</td>\n",
       "      <td>6.277007e-07</td>\n",
       "      <td>5.107037e-10</td>\n",
       "      <td>2.767721e-07</td>\n",
       "      <td>1.254552e-13</td>\n",
       "      <td>1.645351e-13</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.997549e-01</td>\n",
       "      <td>9.784114e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.552714e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.941168e-02</td>\n",
       "      <td>3.799459e-02</td>\n",
       "      <td>6.661338e-15</td>\n",
       "      <td>6.778254e-10</td>\n",
       "      <td>1.301085e-04</td>\n",
       "      <td>1.097661e-01</td>\n",
       "      <td>3.081024e-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.405418</td>\n",
       "      <td>8.858025e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        android            c#           c++          html           ios  \\\n",
       "0  3.461698e-11  6.277007e-07  5.107037e-10  2.767721e-07  1.254552e-13   \n",
       "1  0.000000e+00  9.997549e-01  9.784114e-07  0.000000e+00  3.552714e-15   \n",
       "2  0.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00   \n",
       "3  4.941168e-02  3.799459e-02  6.661338e-15  6.778254e-10  1.301085e-04   \n",
       "4  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "\n",
       "           java    javascript  jquery       php        python  \n",
       "0  1.645351e-13  0.000000e+00     0.0  1.000000  0.000000e+00  \n",
       "1  0.000000e+00  1.000000e+00     0.0  0.000000  0.000000e+00  \n",
       "2  1.000000e+00  0.000000e+00     0.0  0.000000  0.000000e+00  \n",
       "3  1.097661e-01  3.081024e-11     0.0  0.405418  8.858025e-12  \n",
       "4  0.000000e+00  0.000000e+00     0.0  0.000000  0.000000e+00  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_loss = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# классификатор с параметрами по умолчанию\n",
    "# (регуляризация и подсчет частотного словаря активированы)\n",
    "model_w_freq = OnlineLogisticRegression(top_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_filepath) as train_file:\n",
    "    model_w_freq.fit(train_file, total=train_lines_count, return_train_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_loss['lmbda=0.0002']= model_w_freq.loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отфильтруем словарь топ-10k словами и выполним еще одну итерацию обучения\n",
    "model_w_freq.filter_vocab(10000)\n",
    "\n",
    "with open(train_filepath) as train_file:\n",
    "    model_w_freq.fit(train_file, total=train_lines_count, update_vocab=False, return_train_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_loss['lmbda=0.0002, top-10k']= model_w_freq.loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  отфильтруем словарь топ-5k словами и выполним еще одну итерацию обучения\n",
    "model_w_freq.filter_vocab(5000)\n",
    "\n",
    "with open(train_filepath) as train_file:\n",
    "    model_w_freq.fit(train_file, total=train_lines_count, update_vocab=False, return_train_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_loss['lmbda=0.0002, top-5k']= model_w_freq.loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# классификатор без регуляризации и частотного словаря\n",
    "model_wo_freq = OnlineLogisticRegression(top_tags, lmbda=0.0, store_frequency=False)\n",
    "\n",
    "with open(train_filepath) as train_file:\n",
    "    model_wo_freq.fit(train_file, total=train_lines_count, return_train_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_loss['lmbda=0.0, 1st pass']= model_wo_freq.loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# второй проход по выборке\n",
    "with open(train_filepath) as train_file:\n",
    "    model_wo_freq.fit(train_file, total=train_lines_count, return_train_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_loss['lmbda=0.0, 2nd pass']= model_wo_freq.loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll = 1000\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for (key, val) in models_loss.items():\n",
    "    plt.plot(pd.Series(val).rolling(roll).mean(), label=key)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
