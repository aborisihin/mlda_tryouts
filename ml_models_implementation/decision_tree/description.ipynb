{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Деревья решений: классификация и регрессия</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализована модель дерева решений для задач классификации и регрессии. Для построения дерева, по аналогии с реализацией деревьев в scikit-learn, используется модифицированный [алгоритм CART](https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание математической основы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Построение деревьев решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала рассмотрим задачу классификации для объектов только с категориальными признаками и механизм построения решающих деревьев, основанный на концепции информационной энтропии (алгоритм [ID3](https://en.wikipedia.org/wiki/ID3_algorithm)).\n",
    "\n",
    "Энтропия, интуитивно, это мера непоределенности в системе. Итак, энтропия Шеннона для системы с $K$ возможными состояниями определяется как:\n",
    "\n",
    "$$\n",
    "S = - \\sum_{k=1}^{K} p_k \\log_2 p_k\n",
    "$$\n",
    "\n",
    "где $p_i$ - вероятность нахождения системы в $i$-ом состоянии (фактически, доля объектов класса $i$ в выборке):\n",
    "\n",
    "$$\n",
    "p_i = \\dfrac {N_i}{N}\n",
    "$$\n",
    "\n",
    "Это выражение можно трактовать как оценку среднего количества информации, необходимого для определения класса примера из выборки. Максимум энтропии достигается, когда каждое состояние системы равновероятно.\n",
    "\n",
    "Пусть у нас имеется выборка $X$ объектов и вектор ответов $y$.\n",
    "\n",
    "Если выполнить разбиение выборки по признаку $x_j$, то ту же оценку среднего количества информации можно записать как:\n",
    "\n",
    "$$\n",
    "S_j = \\sum_{i=1}^{J} \\frac {N_i} {N} S_i\n",
    "$$\n",
    "\n",
    "где $J$ - число групп после разбиения.\n",
    "\n",
    "В качестве критерия выбора признака разбиения можно ввести понятие прироста информации (information gain, IG):\n",
    "\n",
    "$$\n",
    "IG_j = S - S_j = S - \\sum_{i=1}^{J} \\frac {N_i} {N} S_i\n",
    "$$\n",
    "\n",
    "Тогда выбор лучшего признака для разбиения запишется через следующее выражение:\n",
    "\n",
    "$$\n",
    "Q^{*} = argmax_j \\left( IG_j \\right) = argmax_j \\left( S - \\sum_{i=1}^{J} \\frac {N_i} {N} S_i \\right)\n",
    "$$\n",
    "\n",
    "В основе алгоритма построения деревьев решений ID3 лежит принцип жадной максимизации критерия выбора признака. На каждом шаге разбиения выбирается тот признак, при разделении по которому прирост информации оказывается наибольшим. \n",
    "\n",
    "Разбиение выборки (построение дерева) продолжается рекурсивно до выполнения критерия останова. В базовом случае этим критерием является достижение энтропии в узле дерева значения $0$, что означает нахождение в узле объектов одного класса, или невозможность уменьшить энтропию в узле никаким разбиением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Использование вещественных признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В алгоритме [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm) добавляется возможность работы с вещественными признаками по следующей схеме. Рассмотрим вещественный признак с индексом $j$. Определим пороги разбиения выборки по этому признаку как упорядоченные уникальные значения этого признака на выборке:\n",
    "\n",
    "$$\n",
    "x_j \\in \\left \\{ t_1, t_2, \\dots, t_k \\right \\}_{}\n",
    "$$\n",
    "\n",
    "Тогда, выбрав параметр $\\theta$, выборку можно разделить по предикату $x_j \\le t_m$:\n",
    "\n",
    "$$\n",
    "\\theta = \\left( j, t_m \\right )\n",
    "$$\n",
    "\n",
    "$$\n",
    "X_L(\\theta) = (X, y) \\mid x_j \\le t_m\n",
    "$$\n",
    "\n",
    "$$\n",
    "X_R(\\theta) = (X, y) \\mid x_j > t_m\n",
    "$$\n",
    "\n",
    "И выбор лучшего признака и его порога разбиения будет выглядеть как:\n",
    "\n",
    "$$\n",
    "\\theta^{*} = argmax_\\theta (IG_\\theta) = argmax_\\theta \\left( S - \\dfrac {N_L} {N} S_L - \\dfrac {N_R} {N} S_R \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Использование различных критериев разбиения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы задали функционал для построения деревьев решений на выборках с категориальными и вещественными признаками, который нужно максимизировать на каждом этапе разбиения выборки:\n",
    "\n",
    "$$\n",
    "Q \\left( X, j, t \\right) = F(X) - \\dfrac {\\mid X_L \\mid} {\\mid X \\mid} F(X_L) - \\dfrac {\\mid X_R \\mid} {\\mid X \\mid} F(X_R)\n",
    "$$\n",
    "\n",
    "где критерий качества разбиения $F(X)$ нами был определен как энтропия Шеннона S(X):\n",
    "\n",
    "$$\n",
    "F(X) = S(X) = - \\sum_{k=1}^{K} p_k log_2 p_k\n",
    "$$\n",
    "\n",
    "В задачах классификации часто используется и другой критерий - [неопределенность Джини](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity). Он определяется следующим образом:\n",
    "\n",
    "$$\n",
    "F(X) = 1 - \\sum_{k=1}^{K} p_k^2\n",
    "$$\n",
    "\n",
    "Неопределенность Джини можно трактовать как вероятность ошибки классификации объекта, если классифицировать его случайно в соответствии с распределением классов в узле. Является мерой однородности от $0$ (однородной) до $1$ (гетерогенной)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При решении задачи регрессии идея построения дерева остается той же, но используются другие критерии качества разбиения. Они способствует выбору таких параметров разбиения, при которых значения признаков в подвыборках примерно равны.\n",
    "\n",
    "Дисперсия ответов вокруг среднего:\n",
    "\n",
    "$$\n",
    "F(X) = D(X) = \\dfrac {1}{N} \\sum_{i=1}^{N} \\left( y_i - \\dfrac {1}{N} \\sum_{j=1}^{N} y_j \\right) ^ 2\n",
    "$$\n",
    "\n",
    "Среднее отклонение ответов от медианы:\n",
    "\n",
    "$$\n",
    "F(X) = \\dfrac {1}{N} \\sum_{i=1}^{N} \\mid y_i - med(y) \\mid\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ответы в листах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве ответов в листьях дерева будут приниматься: наиболее многочисленный класс у объектов в листе - для задачи классификации, среднее значение ответов по объектам листа - для задачи регрессии:\n",
    "\n",
    "$$\n",
    "prediction_C(X) = argmax_k (p_k) = argmax_k \\left( \\dfrac {N_k}{N} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "prediction_R(X) = \\dfrac {1}{N} \\sum_{i=1}^{N} y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Алгоритм CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм построения деревьев решений CART похож на C4.5, но имеются отличия. Он поддреживает задачу регрессии по вышеописанному механизму и строит бинарные деревья, т.е. все разбиения в узлах выполняются по предикату: \n",
    "\n",
    "$$\n",
    "\\theta(j, t_m): x_j \\le t_m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используемые источники"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Энтропия и деревья принятия решений https://habr.com/post/171759/\n",
    "2. Открытый курс машинного обучения. Тема 3. Классификация, деревья решений и метод ближайших соседей https://habr.com/company/ods/blog/322534/\n",
    "3. Scikit-learn Decision Trees http://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
