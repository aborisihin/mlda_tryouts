{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Деревья решений: классификация и регрессия</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Автор материала: Алексей Борисихин"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализована модель дерева решений для задач классификации и регрессии. Для построения дерева, по аналогии с реализацией деревьев в scikit-learn, используется модифицированный [алгоритм CART](https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание математической основы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Построение деревьев решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала рассмотрим задачу классификации для объектов только с категориальными признаками и механизм построения решающих деревьев, основанный на концепции информационной энтропии (алгоритм [ID3](https://en.wikipedia.org/wiki/ID3_algorithm)).\n",
    "\n",
    "Пусть у нас имеется выборка $X$ объектов и вектор ответов $y$.\n",
    "\n",
    "Энтропия, интуитивно, это мера непоределенности в системе. Итак, энтропия Шеннона для системы с $K$ возможными состояниями определяется как:\n",
    "\n",
    "$$\n",
    "S = - \\sum_{k=1}^{K} p_k \\log_2 p_k\n",
    "$$\n",
    "\n",
    "где $p_k$ - вероятность нахождения системы в $k$-ом состоянии (фактически, доля объектов класса $k$ в выборке):\n",
    "\n",
    "$$\n",
    "p_k = \\dfrac {N_k}{N} = \\dfrac {\\sum_i [y_i = k]}{\\mid y \\mid}\n",
    "$$\n",
    "\n",
    "Это выражение можно трактовать как оценку среднего количества информации, необходимого для определения класса примера из выборки. Максимум энтропии достигается, когда каждое состояние системы равновероятно.\n",
    "\n",
    "Если выполнить разбиение выборки по категориальному признаку $x_j$, то ту же оценку среднего количества информации можно записать как:\n",
    "\n",
    "$$\n",
    "S_j = \\sum_{i=1}^{J} \\frac {N_i} {N} S_i\n",
    "$$\n",
    "\n",
    "где $J$ - число групп после разбиения, $S_i$ - энтропия в каждой подвыборке.\n",
    "\n",
    "В качестве критерия выбора признака разбиения можно ввести понятие прироста информации (information gain, IG):\n",
    "\n",
    "$$\n",
    "IG_j = S - S_j = S - \\sum_{i=1}^{J} \\frac {N_i} {N} S_i\n",
    "$$\n",
    "\n",
    "Тогда выбор индекса лучшего признака для разбиения запишется через следующее выражение:\n",
    "\n",
    "$$\n",
    "j^{*} = argmax_j \\left( IG_j \\right) = argmax_j \\left( S - \\sum_{i=1}^{J} \\frac {N_i} {N} S_i \\right)\n",
    "$$\n",
    "\n",
    "В основе алгоритма построения деревьев решений ID3 лежит принцип жадной максимизации критерия выбора признака. На каждом шаге разбиения выбирается тот признак, при разделении по которому прирост информации оказывается наибольшим. \n",
    "\n",
    "Разбиение выборки (построение дерева) продолжается рекурсивно до выполнения критерия останова. В базовом случае этим критерием является достижение энтропии в узле дерева значения $0$, что означает нахождение в узле объектов одного класса, или невозможность уменьшить энтропию в узле никаким разбиением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Использование вещественных признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В алгоритмах CART и [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm) добавляется возможность работы с вещественными признаками по следующей схеме. Рассмотрим вещественный признак с индексом $j$. Определим пороги разбиения выборки по этому признаку как упорядоченные уникальные значения этого признака на выборке:\n",
    "\n",
    "$$\n",
    "x_j \\in \\left \\{ t_1, t_2, \\dots, t_k \\right \\}_{}\n",
    "$$\n",
    "\n",
    "Тогда, выбрав параметр $\\theta$, выборку можно разделить по предикату $x_j \\le t_m$:\n",
    "\n",
    "$$\n",
    "\\theta = \\left( j, t_m \\right )\n",
    "$$\n",
    "\n",
    "$$\n",
    "X_L(\\theta) = (X, y) \\mid x_j \\le t_m\n",
    "$$\n",
    "\n",
    "$$\n",
    "X_R(\\theta) = (X, y) \\mid x_j > t_m\n",
    "$$\n",
    "\n",
    "И выбор лучшего признака и его порога разбиения будет выглядеть как:\n",
    "\n",
    "$$\n",
    "\\theta^{*} = argmax_\\theta (IG_\\theta) = argmax_\\theta \\left( S - \\dfrac {N_L} {N} S_L - \\dfrac {N_R} {N} S_R \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Уточнение механизма обработки категориальных признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В некоторых подходах бинарной классификации обработку категориальных признаков без предобработки их значений (LabelEncoder и т.д.)  предлагается проводить по следующей схеме. Первым шагом выполняется упорядочивание значений категориального признака $x_j$ на основе того, какая доля объектов обучающей выборки с данным значением признака придалежит классу \"+1\":\n",
    "\n",
    "$$\n",
    "x_j \\in \\{ q_1, q_2, \\dots, q_k \\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dfrac {1}{N_{q_1}} \\sum_{x \\in X(x_j = q_1)} [ y = +1 ] \\le \\dots \\le \\dfrac {1}{N_{q_k}} \\sum_{x \\in X(x_j = q_k)} [ y = +1 ]\n",
    "$$\n",
    "\n",
    "Этим упорядочиванием мы вводим естественный порядок значений категориального признака и можем заменить значения $q_i$ на число $i$. Теперь разбиения вершины дерева можно выполнять по предикату $x_j \\le i$.\n",
    "\n",
    "В рамках работы предполагается поддержка многоклассовой задачи классификации, и данный подход оказывается неприменим. Категориальные признаки поддерживаться не будут, необходима их предобработка до получения бинарных признаков (применение техник, подобных OneHotEncoder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Использование различных критериев разбиения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы задали функционал для построения деревьев решений на выборках с категориальными и вещественными признаками, который нужно максимизировать на каждом этапе разбиения выборки:\n",
    "\n",
    "$$\n",
    "Q \\left( X, j, t \\right) = F(X) - \\dfrac {\\mid X_L \\mid} {\\mid X \\mid} F(X_L) - \\dfrac {\\mid X_R \\mid} {\\mid X \\mid} F(X_R)\n",
    "$$\n",
    "\n",
    "где критерий качества разбиения $F(X)$ нами был определен как энтропия Шеннона S(X):\n",
    "\n",
    "$$\n",
    "F(X) = S(X) = - \\sum_{k=1}^{K} p_k log_2 p_k\n",
    "$$\n",
    "\n",
    "В задачах классификации часто используется и другой критерий - [неопределенность Джини](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity). Он определяется следующим образом:\n",
    "\n",
    "$$\n",
    "F(X) = 1 - \\sum_{k=1}^{K} p_k^2\n",
    "$$\n",
    "\n",
    "Неопределенность Джини можно трактовать как вероятность ошибки классификации объекта, если классифицировать его случайно в соответствии с распределением классов в узле. Является мерой однородности от $0$ (однородной) до $1$ (гетерогенной)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При решении задачи регрессии идея построения дерева остается той же, но используются другие критерии качества разбиения. Они способствуют выбору таких параметров разбиения, при которых значения признаков в подвыборках примерно равны.\n",
    "\n",
    "Дисперсия ответов вокруг среднего:\n",
    "\n",
    "$$\n",
    "F(X) = D(X) = \\dfrac {1}{N} \\sum_{i=1}^{N} \\left( y_i - \\dfrac {1}{N} \\sum_{j=1}^{N} y_j \\right) ^ 2\n",
    "$$\n",
    "\n",
    "Среднее отклонение ответов от медианы:\n",
    "\n",
    "$$\n",
    "F(X) = \\dfrac {1}{N} \\sum_{i=1}^{N} \\mid y_i - med(y) \\mid\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ответы в листах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве ответов в листьях дерева будут приниматься: наиболее многочисленный класс у объектов в листе - для задачи классификации, среднее значение ответов по объектам листа - для задачи регрессии:\n",
    "\n",
    "$$\n",
    "prediction_C(X) = argmax_k (p_k) = argmax_k \\left( \\dfrac {\\sum_{i=1}^{N} [y_i = k]}{\\mid y \\mid} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "prediction_R(X) = \\dfrac {1}{N} \\sum_{i=1}^{N} y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Работа с пропусками в данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из преимуществ деревьев решений является их способность работать с пропущенными данными. Рассмотрим схемы, применяемые в алгоритмах построения деревьев решений.\n",
    "\n",
    "Пусть на стадии обучения во время поиска оптимального разбиения в вершине встречается признак с пропущенными значениями. Обозначим подвыборку объектов с пропущенными значениями признака $x_j$ как $V_j$. Тогда при расчете функционала мы можем просто проигнорировать эти объекты, внося поправку на потерю информации от этого:\n",
    "\n",
    "$$\n",
    "Q'(X, j, t) \\approx \\dfrac {\\mid X \\backslash V_j \\mid}{\\mid X \\mid} Q(X \\backslash V_j, j ,t)\n",
    "$$\n",
    "\n",
    "Если в качестве оптимального разбиения будет выбран предикат с рассмотренным нами признаком, то перед нами встает вопрос, каким образом распределить по дочерним вершинам объекты с пропущенными значениями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "В алгоритме C4.5 предлагается включать объекты из $V_j$ как в левый, так и в правый дочерний узел, но с соответствующими весами:\n",
    "\n",
    "$$\n",
    "w_L = \\dfrac {\\mid X_L \\mid}{\\mid X \\mid}; \\: w_R = \\dfrac {\\mid X_R \\mid}{\\mid X \\mid}\n",
    "$$\n",
    "\n",
    "Эти веса в дальнейших операциях будут использоваться как коэффициенты перед индикаторами $[y_i = k]$ в задаче классификации.\n",
    "\n",
    "На стадии прогнозирования, если объект попал в вершину, предикат которой не может быть вычислен из-за пропусков в данных, то прогноз для объекта вычисляется как сумма прогнозов обоих поддеревьев этой вершины с весами, пропорциональными количеству объектов в поддеревьях:\n",
    "\n",
    "$$\n",
    "prediction(x) = \\dfrac {\\mid X_L \\mid}{\\mid X \\mid} prediction_L(x) + \\dfrac {\\mid X_R \\mid}{\\mid X \\mid} prediction_R(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Алгоритм CART предлагает использовать метод суррогатных разбиений. Этим методом будем пользоваться и мы. Основной идеей метода является поиск такого альтернативного (суррогатного) разбиения, которое максимально похоже на выбранное нами оптимальное. Мера похожести вычисляется следующим образом:\n",
    "\n",
    "$$\n",
    "similarity = \\dfrac {\\mid X_L^* \\cap X_L^{'} \\mid}{\\mid X \\mid} + \\dfrac {\\mid X_R^* \\cap X_R^{'} \\mid}{\\mid X \\mid}\n",
    "$$\n",
    "\n",
    "где $\\left( X_L^*, X_R^* \\right)$ - оптимальное разбиение, $\\left( X_L^{'}, X_R^{'} \\right)$ - суррогатное разбиение.\n",
    "\n",
    "В случае, когда найденное значение $similarity < 0.5$, достаточно в суррогатном разбиении поменять местами левую и правую ветвь и заменить значение на $(1 - similarity)$.\n",
    "\n",
    "Также вводится разбиение \"по большинству\" (пример отправляется в ветвь, содержащую большую часть выборки). Его мера похожести определяется как \n",
    "\n",
    "$$\n",
    "similarity = max \\left( \\dfrac {\\mid X_L^* \\mid}{\\mid X \\mid}, \\dfrac {\\mid X_R^* \\mid}{\\mid X \\mid} \\right)\n",
    "$$\n",
    "\n",
    "и она является порогом, по которому отбрасываются суррогатные разбиения с меньшим значением $similarity$.\n",
    "\n",
    "Все найденные суррогатные разбиения ранжируются и сохраняются для каждой вершины дерева. Если встречаются примеры выборки с пропущенным значением основного предиката разбиения, то поочередно проверяются суррогатные предикаты, пока не будет найден такой с непропущенным значением признака. Если этого не произошло, применяется разбиение \"по большинству\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Файлы проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>dt_class.py</b>\n",
    "\n",
    "    Объявление и реализация класса модели.<br><br>\n",
    "    \n",
    "- <b>estimator_test.ipynb</b>\n",
    "\n",
    "    Notebook, содержащий тестирование класса модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для тестирования полученной модели и сравнения ее с деревьями решений из библиотеки sklearn были использованы средства этой библиотеки для генерации датасетов: [make_classification](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) и [make_regression](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html). Тестирование для задач классификации и регрессии было проведено в три этапа: \n",
    "- с помощью кросс-валидации были получены показатели качества (ROC-AUC и MSE) для разных величин максимальной глубины дерева;\n",
    "- также с помощью кросс-валидации проводился замер качества полученной модели на датасетах с разной долей пропущенных значений (деревья sklearn не работают с ними);\n",
    "- сравнивалось качество суррогатных и константных разбиений на тестовых датасетах с разной долей пропущенных значений (обучение было проведено на датасете без пропусков в данных).\n",
    "\n",
    "Сразу хочется отметить разницу в скорости работы алгоритмов. Классы деревьев sklearn превосходят полученный алгоритм почти на два порядка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задаче классификации был сгенерирован датасет размером 1000 примеров со следующими характеристиками: число информативных признаков - 5, число избыточных признаков (линейные комбинации информативных) - 3, число случайно сгенерированных шумовых признаков - 2, число кластеров для каждого из классов - 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Созданный класс сравнивался с классом [sklearn.tree.DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). Оба объекта были созданы с параметрами по-умолчанию и критерием разбиения Джини. На датасете без пропусков выполнялся поиск оптимального значения глубины дерева на интервале \\[2, 15\\] с 5-кратной кросс-валидацией для каждого значения параметра. Графики зависимости качества классификации (был выбран критерий ROC-AUC) от значения максимальной глубины дерева:\n",
    "<img src='./img/comparsion_gini.png'>\n",
    "Можно наблюдать более высокое качество классификации на деревьях малой глубины у класса sklearn. При росте глубины дерева значения качества классификации сближаются. У полученной модели график на тестовой подвыборке выглядит более сглаженным и с меньшим значением стандартного отклонения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем те же операции для деревьев с энтропийным критерием разбиения.\n",
    "<img src='./img/comparsion_entropy.png'>\n",
    "Картина осталась похожей на предыдущий случай, плюс сглаженность и меньшее значение стандартного отклонения на тестовой подвыборке стали более выраженными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдем ко второму шагу тестирования, проверим работу моделей на данных с пропусками. Т.к. деревья решений sklearn такую возмоность не поддерживают, остановимся на созданной модели. В качестве критерия разбиения будем использовать gini, а глубину ограничим значением 7. Пропуски будем генерировать случайно, во всех признаках выборки, с разными значениями доли от общего объема: от 0 до 0.5 с шагом 0.05. Для каждого значения доли проведем 5-кратную кросс-валидацию для двух версий дерева: с константным распределением пропусков (при каждом разбиении относятся к одной и той же ветви) и с использованием суррогатных разбиений.\n",
    "<img src='./img/missing_classification.png'>\n",
    "Суррогатные разбиения в среднем дают лучший результат, но ненамного, в рамках стандартного отклонения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Третий шаг тестирования. Проверим работу моделей на данных с пропусками по другой схеме. Оставим значения долей пропусков и версии дерева, но теперь обучение будем производить на тренировочной части без пропусков в данных, а тестирование - на части с пропусками.\n",
    "<img src='./img/test_missing_classification.png'>\n",
    "В этом случае разница уже ощутима."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдем к задаче регрессии. Был сгенерирован датасет из 1000 примеров с параметрами: информатвных признаков - 7, избыточных - 3, величина стандартного отклонения гауссовского шума, добавленного к данным - 0.05. Проведем те же операции, что и в задаче классификации. Критерием качества решения задачи будет служить MSE.\n",
    "\n",
    "Для сравнения были взяты классы с параметрами по-умолчанию: [sklearn.tree.DecisionTreeRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor) с критерием MSE и два полученных дерева, с критерием разбиения variance и mad_median.\n",
    "\n",
    "Результат поиска оптимальной глубины дерева.\n",
    "<img src='./img/comparsion_regression.png'>\n",
    "Картина похожа на случай классификации. Такое же преимущество у дерева sklearn с небольшим значением глубины, и также близкими значения качества на больших значениях глубины (только теперь сходство почти полное)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Повторим второй шаг тестирования. Рассмотрим полученную модель с критерием разбиения variance и максимальной глубиной дерева 8.\n",
    "<img src='./img/missing_regression.png'>\n",
    "Опять, как и в случае классификации, качество решения задачи в среднем больше у дерева с суррогатными разбиениями. Но на доле пропусков 0.3 и выше разница в качестве становится больше величины стандартного отклонения, что позволяет говорить об улучшении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И, наконец, третий шаг. Обучим регрессоры на подвыборке без пропусков, а проверим на подвыборке с пропущенными значениями.\n",
    "<img src='./img/test_missing_regression.png'>\n",
    "В этом случае разницу в качестве можно назвать существенной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используемые источники"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Энтропия и деревья принятия решений https://habr.com/post/171759/\n",
    "2. Открытый курс машинного обучения. Тема 3. Классификация, деревья решений и метод ближайших соседей https://habr.com/company/ods/blog/322534/\n",
    "3. Решающие деревья (лекция ФКН ВШЭ) https://www.hse.ru/mirror/pubs/share/215285956\n",
    "3. Scikit-learn Decision Trees http://scikit-learn.org/stable/modules/tree.html\n",
    "4. An Introduction to Recursive Partitioning Using the RPART Routines https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf\n",
    "5. Decision Tree: Review of Techniques for Missing Values at Training, Testing and Compatibility http://uksim.info/aims2015/CD/data/8675a122.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
