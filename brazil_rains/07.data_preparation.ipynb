{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T20:01:26.630941Z",
     "start_time": "2018-04-22T20:01:26.587942Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T19:36:37.818808Z",
     "start_time": "2018-04-22T19:36:37.802608Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROCESSED_DATA_FILE = './../data/project_brazil/processed.csv'\n",
    "\n",
    "RANDOM_STATE = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T19:36:50.151154Z",
     "start_time": "2018-04-22T19:36:37.823793Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(PROCESSED_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T19:36:50.171274Z",
     "start_time": "2018-04-22T19:36:50.151154Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prcp_transform(val):\n",
    "    if val == 0.0:\n",
    "        return 0\n",
    "    elif (val > 0.0) & (val <= 1.2):\n",
    "        return 1\n",
    "    elif (val > 1.2) & (val <=2.5):\n",
    "        return 2\n",
    "    elif (val > 2.5):\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Подготовим датасет для дальнейшей работы моделей. Начнем с целевой переменной. Выполним фильтрацию всего датасета по 99% квантилю целевой перемнной. Выполним сдвиг ее значений на одну позицию вперед, чтобы прогнозировать осадки на час, следующий за наблюдаемым, и выкинем из датасета первую строчку, у которой значение признака **prcp** станет пустым."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T19:36:51.360702Z",
     "start_time": "2018-04-22T19:36:50.178267Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prcp_quantile = df['prcp'].quantile(0.99)\n",
    "df = df[df['prcp'] < prcp_quantile]\n",
    "\n",
    "df['prcp'] = df['prcp'].shift(1)\n",
    "df.drop(index=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним кодирование значений целевой переменной и выделим ее в отдельный объект."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T19:36:52.176000Z",
     "start_time": "2018-04-22T19:36:51.360702Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df['prcp'].apply(prcp_transform).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим словарь обработанных признаков, для дальнейшего их объединения в итоговый датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T20:10:32.179304Z",
     "start_time": "2018-04-22T20:10:32.174315Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_features = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Т.к. одной из моделей у нас является линейная, то категориальные признаки нам необходимо закодировать, а численные - отмасштабировать.\n",
    "<p>Обозначим схему преобразования данных. В нашей задаче у нас отсутствует тестовая выборка, валидацию моделей будем проводить на отложенной части выборки. Но преобразовывать признаки мы будем на всей имеющейся выборке сразу. В общем случае это неправильно, т.к. из валидационной части может \"просочиться\" информация в обучающую часть (например, учтется масштаб всех значений признаков, будут закодированы все значения категориальных признаков, и т.д.). Мы же закроем глаза на это допущение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признак **wsid**. Является категориальным, выполним One-Hot-Encoding с выбрасыванием первой колонки (для того, чтобы не возникало зависимости в новых категориальных признаках). Признаки **wsnm, inme** не используем как дублирующие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T20:10:33.370688Z",
     "start_time": "2018-04-22T20:10:32.995760Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wsid_ohe = pd.get_dummies(df['wsid'], prefix='wsid_ohe', drop_first=True)\n",
    "processed_features['wsid_ohe'] = wsid_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признаки **elvt, lat, lon** являются численными, выполним масштабирование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T20:10:33.678655Z",
     "start_time": "2018-04-22T20:10:33.373693Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stnd_scaler = StandardScaler()\n",
    "\n",
    "elvt_scaled = stnd_scaler.fit_transform(df[['elvt']])\n",
    "processed_features['elvt_scaled'] = pd.DataFrame(elvt_scaled, columns=['elvt_scaled'])\n",
    "\n",
    "lat_scaled = stnd_scaler.fit_transform(df[['lat']])\n",
    "processed_features['lat_scaled'] = pd.DataFrame(lat_scaled, columns=['lat_scaled'])\n",
    "\n",
    "lon_scaled = stnd_scaler.fit_transform(df[['lon']])\n",
    "processed_features['lon_scaled'] = pd.DataFrame(lon_scaled, columns=['lon_scaled'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признаки **city, prov** не будем использовать в моделях, как малоинформативные (географическую информацию мы уже добавили в виде широты и долготы), но потенциально \"раздувающие\" датасет (они категориальны и придется кодировать)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как было обозначено ранее, из признаков даты и времени мы будем использовать только **mo** (месяц). Пояснения см. в п. 4. Несмотря на целочисленность признака, он является конечно же категориальным, так что выполним One-Hot-Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T20:10:34.013449Z",
     "start_time": "2018-04-22T20:10:33.871511Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mo_ohe = pd.get_dummies(df['mo'], prefix='mo_ohe', drop_first=True)\n",
    "processed_features['mo_ohe'] = mo_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для **stp, temp, dewp** (давления, температуры воздуха, температуры точки росы) возьмем только мгновенные их значения (максимальные и минимальные значения линейно зависимы, см. п. 4). Воспользуемся масштабированием. Пустые значения заменим средними."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T20:10:34.778589Z",
     "start_time": "2018-04-22T20:10:34.360457Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stp_scaled = stnd_scaler.fit_transform(df[['stp']])\n",
    "processed_features['stp_scaled'] = pd.DataFrame(stp_scaled, columns=['stp_scaled'])\n",
    "\n",
    "temp_scaled = stnd_scaler.fit_transform(df[['temp']].fillna(df[['temp']].mean()))\n",
    "processed_features['temp_scaled'] = pd.DataFrame(temp_scaled, columns=['temp_scaled'])\n",
    "\n",
    "dewp_scaled = stnd_scaler.fit_transform(df[['dewp']].fillna(df[['dewp']].mean()))\n",
    "processed_features['dewp_scaled'] = pd.DataFrame(dewp_scaled, columns=['dewp_scaled'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для **gbrd** (солнечное излучение) заполним пропуски и выполним масштабирование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T20:10:35.113789Z",
     "start_time": "2018-04-22T20:10:34.947856Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbrd_scaled = stnd_scaler.fit_transform(df[['gbrd']].fillna(df[['gbrd']].mean()))\n",
    "processed_features['gbrd_scaled'] = pd.DataFrame(gbrd_scaled, columns=['gbrd_scaled'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Показатели относительной влажности (мгновенная, максимальная и минимальная) используем все, т.к. линейной зависимости в них не наблюдалось, и данные могут быть полезными для прогнозирования осадков. Воспользуемся масштабированием. Пустые значения заменим средними."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T20:10:35.844473Z",
     "start_time": "2018-04-22T20:10:35.475135Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hmdy_scaled = stnd_scaler.fit_transform(df[['hmdy']])\n",
    "processed_features['hmdy_scaled'] = pd.DataFrame(hmdy_scaled, columns=['hmdy_scaled'])\n",
    "\n",
    "hmin_scaled = stnd_scaler.fit_transform(df[['hmin']].fillna(df[['hmin']].mean()))\n",
    "processed_features['hmin_scaled'] = pd.DataFrame(hmin_scaled, columns=['hmin_scaled'])\n",
    "\n",
    "hmax_scaled = stnd_scaler.fit_transform(df[['hmax']].fillna(df[['hmax']].mean()))\n",
    "processed_features['hmax_scaled'] = pd.DataFrame(hmax_scaled, columns=['hmax_scaled'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Показатели ветра **wdsp, gust** (скорость, порывы) отмасштабируем и заполним пропуски. Направление ветра **wdct** разобьем на 10-градусные сектора, 36-ой сектор заменим нулевым, и выполним One-Hot-Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T20:10:38.146839Z",
     "start_time": "2018-04-22T20:10:36.107346Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wdsp_scaled = stnd_scaler.fit_transform(df[['wdsp']].fillna(df[['wdsp']].mean()))\n",
    "processed_features['wdsp_scaled'] = pd.DataFrame(wdsp_scaled, columns=['wdsp_scaled'])\n",
    "\n",
    "gust_scaled = stnd_scaler.fit_transform(df[['gust']].fillna(df[['gust']].mean()))\n",
    "processed_features['gust_scaled'] = pd.DataFrame(gust_scaled, columns=['gust_scaled'])\n",
    "\n",
    "wdct_processed = df['wdct'].fillna(df[['wdct']].mean())\n",
    "wdct_processed = wdct_processed.apply(lambda x: x // 10).apply(lambda x: x if x != 36.0 else 0.0)\n",
    "wdct_ohe = pd.get_dummies(wdct_processed, prefix='wdct_ohe', drop_first=True)\n",
    "processed_features['wdct_ohe'] = wdct_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, соберем все признаки воедино."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T20:10:53.265335Z",
     "start_time": "2018-04-22T20:10:52.688086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обработанной выборки: (1393614, 179)\n"
     ]
    }
   ],
   "source": [
    "for name_df, part_df in processed_features.items():\n",
    "    processed_features[name_df] = part_df.reset_index(drop=True)\n",
    "\n",
    "X = pd.concat(processed_features.values(), axis=1)\n",
    "print('Размер обработанной выборки: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем отбор признаков. Для начала обучим логистическую регрессию на всей выборке с L1-регуляризатором. Этот тип регуляризации, также называемый Lasso, характеризуется тем, что обнуляет веса у наименее значимых признаков. И чем больше параметр регуляризации, тем больше будут обнуляться веса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-04-22T20:12:09.758Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_l1_model = LogisticRegression(penalty='l1', C=1.0, random_state=RANDOM_STATE)\n",
    "lr_l1_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-04-22T20:12:25.853Z"
    }
   },
   "outputs": [],
   "source": [
    "zip(X.columns, lr_l1_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим обучающую и валидационную части выборки. Ранее уже упоминалось, что упорядочивание выборки по времени не имеет смысла, так что воспользуемся случайным разбиением и выделим для валидации 30% всей выборки. Укажем параметр stratify для сохранения баланса классов в новых выборках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T19:37:02.908171Z",
     "start_time": "2018-04-22T19:36:58.001484Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T19:37:02.927167Z",
     "start_time": "2018-04-22T19:37:02.908171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: (975529, 194)\n",
      "Размер обучающего вектора целевой переменной: (975529,)\n",
      "Размер валидационной выборки: (418085, 194)\n",
      "Размер валидационного вектора целевой переменной: (418085,)\n"
     ]
    }
   ],
   "source": [
    "print('Размер обучающей выборки:', X_train.shape)\n",
    "print('Размер обучающего вектора целевой переменной:', y_train.shape)\n",
    "print('Размер валидационной выборки:', X_test.shape)\n",
    "print('Размер валидационного вектора целевой переменной:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
